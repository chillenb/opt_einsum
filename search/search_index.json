{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Overview","text":"<p>Optimized einsum can significantly reduce the overall execution time of einsum-like expressions by optimizing the expression's contraction order and dispatching many operations to canonical BLAS, cuBLAS, or other specialized routines. Optimized einsum is agnostic to the backend and can handle NumPy, Dask, PyTorch, Tensorflow, CuPy, Sparse, Theano, JAX, and Autograd arrays as well as potentially any library which conforms to a standard API.</p>"},{"location":"#features","title":"Features","text":"<p>The algorithms found in this repository often power the <code>einsum</code> optimizations in many of the above projects. For example, the optimization of <code>np.einsum</code> has been passed upstream and most of the same features that can be found in this repository can be enabled with <code>numpy.einsum(..., optimize=True)</code>. However, this repository often has more up to date algorithms for complex contractions. Several advanced features are as follows:</p> <ul> <li>Inspect detailed information about the path chosen.</li> <li>Perform contractions with numerous backends, including on the GPU and with libraries such as TensorFlow and PyTorch.</li> <li>Generate reusable expressions, potentially with constant tensors, that can be compiled for greater performance.</li> <li>Use an arbitrary number of indices to find contractions for hundreds or even thousands of tensors.</li> <li>Share intermediate computations among multiple contractions.</li> <li>Compute gradients of tensor contractions using Autograd or JAX.</li> </ul>"},{"location":"#example","title":"Example","text":"<p>Take the following einsum-like expression:</p> \\[ M_{pqrs} = C_{pi} C_{qj} I_{ijkl} C_{rk} C_{sl} \\] <p>and consider two different algorithms:</p> <pre><code>import numpy as np\n\ndim = 10\nI = np.random.rand(dim, dim, dim, dim)\nC = np.random.rand(dim, dim)\n\ndef naive(I, C):\n    # N^8 scaling\n    return np.einsum('pi,qj,ijkl,rk,sl-&gt;pqrs', C, C, I, C, C)\n\ndef optimized(I, C):\n    # N^5 scaling\n    K = np.einsum('pi,ijkl-&gt;pjkl', C, I)\n    K = np.einsum('qj,pjkl-&gt;pqkl', C, K)\n    K = np.einsum('rk,pqkl-&gt;pqrl', C, K)\n    K = np.einsum('sl,pqrl-&gt;pqrs', C, K)\n    return K\n</code></pre> <pre><code>&gt;&gt;&gt; np.allclose(naive(I, C), optimized(I, C))\nTrue\n</code></pre> <p>Most einsum functions do not consider building intermediate arrays; therefore, helping einsum functions by creating these intermediate arrays can result in considerable cost savings even for small N (N=10):</p> <pre><code>%timeit naive(I, C)\n1 loops, best of 3: 829 ms per loop\n\n%timeit optimized(I, C)\n1000 loops, best of 3: 445 \u00b5s per loop\n</code></pre> <p>The index transformation is a well-known contraction that leads to straightforward intermediates. This contraction can be further complicated by considering that the shape of the C matrices need not be the same, in this case, the ordering in which the indices are transformed matters significantly. Logic can be built that optimizes the order; however, this is a lot of time and effort for a single expression.</p> <p>The <code>opt_einsum</code> package is a typically a drop-in replacement for <code>einsum</code> functions and can handle this logic and path finding for you:</p> <pre><code>from opt_einsum import contract\n\ndim = 30\nI = np.random.rand(dim, dim, dim, dim)\nC = np.random.rand(dim, dim)\n\n%timeit optimized(I, C)\n10 loops, best of 3: 65.8 ms per loop\n\n%timeit contract('pi,qj,ijkl,rk,sl-&gt;pqrs', C, C, I, C, C)\n100 loops, best of 3: 16.2 ms per loop\n</code></pre> <p>The above will automatically find the optimal contraction order, in this case, identical to that of the optimized function above, and compute the products for you. Additionally, <code>contract</code> can use vendor BLAS with the <code>numpy.dot</code> function under the hood to exploit additional parallelism and performance.</p> <p>Details about the optimized contraction order can be explored:</p> <pre><code>&gt;&gt;&gt; import opt_einsum as oe\n\n&gt;&gt;&gt; path_info = oe.contract_path('pi,qj,ijkl,rk,sl-&gt;pqrs', C, C, I, C, C)\n\n&gt;&gt;&gt; print(path_info[0])\n[(0, 2), (0, 3), (0, 2), (0, 1)]\n\n&gt;&gt;&gt; print(path_info[1])\n  Complete contraction:  pi,qj,ijkl,rk,sl-&gt;pqrs\n         Naive scaling:  8\n     Optimized scaling:  5\n      Naive FLOP count:  8.000e+08\n  Optimized FLOP count:  8.000e+05\n   Theoretical speedup:  1000.000\n  Largest intermediate:  1.000e+04 elements\n--------------------------------------------------------------------------------\nscaling   BLAS                  current                                remaining\n--------------------------------------------------------------------------------\n   5      GEMM            ijkl,pi-&gt;jklp                      qj,rk,sl,jklp-&gt;pqrs\n   5      GEMM            jklp,qj-&gt;klpq                         rk,sl,klpq-&gt;pqrs\n   5      GEMM            klpq,rk-&gt;lpqr                            sl,lpqr-&gt;pqrs\n   5      GEMM            lpqr,sl-&gt;pqrs                               pqrs-&gt;pqrs\n</code></pre>"},{"location":"#citation","title":"Citation","text":"<p>If this code has benefited your research, please support us by citing:</p> <p>Daniel G. A. Smith and Johnnie Gray, opt_einsum - A Python package for optimizing contraction order for einsum-like expressions. Journal of Open Source Software, 2018, 3(26), 753</p> <p>DOI: https://doi.org/10.21105/joss.00753</p>"},{"location":"api_reference/","title":"API Documentation","text":""},{"location":"api_reference/#opt_einsumcontract","title":"<code>opt_einsum.contract</code>","text":"<p>Evaluates the Einstein summation convention on the operands. A drop in replacement for NumPy's einsum function that optimizes the order of contraction to reduce overall scaling at the cost of several intermediate arrays.</p> <p>Parameters:</p> Name Type Description Default <code>subscripts</code> <code>Union[str, ArrayType]</code> <p>Specifies the subscripts for summation.</p> required <code>*operands</code> <code>Union[ArrayType, Collection[int]]</code> <p>These are the arrays for the operation.</p> <code>()</code> <code>out</code> <code>Optional[ArrayType]</code> <p>A output array in which set the resulting output.</p> <code>None</code> <code>dtype</code> <code>Optional[str]</code> <p>The dtype of the given contraction, see np.einsum.</p> <code>None</code> <code>order</code> <code>_OrderKACF</code> <p>The order of the resulting contraction, see np.einsum.</p> <code>'K'</code> <code>casting</code> <code>_Casting</code> <p>The casting procedure for operations of different dtype, see np.einsum.</p> <code>'safe'</code> <code>use_blas</code> <code>bool</code> <p>Do you use BLAS for valid operations, may use extra memory for more intermediates.</p> <code>True</code> <code>optimize</code> <code>OptimizeKind</code> <ul> <li>Choose the type of path the contraction will be optimized with</li> <li>if a list is given uses this as the path.</li> <li><code>'optimal'</code> An algorithm that explores all possible ways of contracting the listed tensors. Scales factorially with the number of terms in the contraction.</li> <li><code>'dp'</code> A faster (but essentially optimal) algorithm that uses dynamic programming to exhaustively search all contraction paths without outer-products.</li> <li><code>'greedy'</code> An cheap algorithm that heuristically chooses the best pairwise contraction at each step. Scales linearly in the number of terms in the contraction.</li> <li><code>'random-greedy'</code> Run a randomized version of the greedy algorithm 32 times and pick the best path.</li> <li><code>'random-greedy-128'</code> Run a randomized version of the greedy algorithm 128 times and pick the best path.</li> <li><code>'branch-all'</code> An algorithm like optimal but that restricts itself to searching 'likely' paths. Still scales factorially.</li> <li><code>'branch-2'</code> An even more restricted version of 'branch-all' that only searches the best two options at each step. Scales exponentially with the number of terms in the contraction.</li> <li><code>'auto'</code> Choose the best of the above algorithms whilst aiming to keep the path finding time below 1ms.</li> <li><code>'auto-hq'</code> Aim for a high quality contraction, choosing the best of the above algorithms whilst aiming to keep the path finding time below 1sec.</li> </ul> <code>True</code> <code>memory_limit</code> <code>_MemoryLimit</code> <ul> <li>Give the upper bound of the largest intermediate tensor contract will build.</li> <li>None or -1 means there is no limit.</li> <li><code>max_input</code> means the limit is set as largest input tensor.</li> <li>A positive integer is taken as an explicit limit on the number of elements.</li> </ul> <p>The default is None. Note that imposing a limit can make contractions exponentially slower to perform.</p> <code>None</code> <code>backend</code> <code>BackendType</code> <p>Which library to use to perform the required <code>tensordot</code>, <code>transpose</code> and <code>einsum</code> calls. Should match the types of arrays supplied, See <code>contract_expression</code> for generating expressions which convert numpy arrays to and from the backend library automatically.</p> <code>'auto'</code> <p>Returns:</p> Type Description <code>ArrayType</code> <p>The result of the einsum expression.</p> Notes <p>This function should produce a result identical to that of NumPy's einsum function. The primary difference is <code>contract</code> will attempt to form intermediates which reduce the overall scaling of the given einsum contraction. By default the worst intermediate formed will be equal to that of the largest input array. For large einsum expressions with many input arrays this can provide arbitrarily large (1000 fold+) speed improvements.</p> <p>For contractions with just two tensors this function will attempt to use NumPy's built-in BLAS functionality to ensure that the given operation is performed optimally. When NumPy is linked to a threaded BLAS, potential speedups are on the order of 20-100 for a six core machine.</p> Source code in <code>opt_einsum/contract.py</code> <pre><code>def contract(\n    subscripts: Union[str, ArrayType],\n    *operands: Union[ArrayType, Collection[int]],\n    out: Optional[ArrayType] = None,\n    dtype: Optional[str] = None,\n    order: _OrderKACF = \"K\",\n    casting: _Casting = \"safe\",\n    use_blas: bool = True,\n    optimize: OptimizeKind = True,\n    memory_limit: _MemoryLimit = None,\n    backend: BackendType = \"auto\",\n    **kwargs: Any,\n) -&gt; ArrayType:\n    \"\"\"\n    Evaluates the Einstein summation convention on the operands. A drop in\n    replacement for NumPy's einsum function that optimizes the order of contraction\n    to reduce overall scaling at the cost of several intermediate arrays.\n\n    Parameters:\n        subscripts: Specifies the subscripts for summation.\n        *operands: These are the arrays for the operation.\n        out: A output array in which set the resulting output.\n        dtype: The dtype of the given contraction, see np.einsum.\n        order: The order of the resulting contraction, see np.einsum.\n        casting: The casting procedure for operations of different dtype, see np.einsum.\n        use_blas: Do you use BLAS for valid operations, may use extra memory for more intermediates.\n        optimize:- Choose the type of path the contraction will be optimized with\n            - if a list is given uses this as the path.\n            - `'optimal'` An algorithm that explores all possible ways of\n            contracting the listed tensors. Scales factorially with the number of\n            terms in the contraction.\n            - `'dp'` A faster (but essentially optimal) algorithm that uses\n            dynamic programming to exhaustively search all contraction paths\n            without outer-products.\n            - `'greedy'` An cheap algorithm that heuristically chooses the best\n            pairwise contraction at each step. Scales linearly in the number of\n            terms in the contraction.\n            - `'random-greedy'` Run a randomized version of the greedy algorithm\n            32 times and pick the best path.\n            - `'random-greedy-128'` Run a randomized version of the greedy\n            algorithm 128 times and pick the best path.\n            - `'branch-all'` An algorithm like optimal but that restricts itself\n            to searching 'likely' paths. Still scales factorially.\n            - `'branch-2'` An even more restricted version of 'branch-all' that\n            only searches the best two options at each step. Scales exponentially\n            with the number of terms in the contraction.\n            - `'auto'` Choose the best of the above algorithms whilst aiming to\n            keep the path finding time below 1ms.\n            - `'auto-hq'` Aim for a high quality contraction, choosing the best\n            of the above algorithms whilst aiming to keep the path finding time\n            below 1sec.\n\n        memory_limit:- Give the upper bound of the largest intermediate tensor contract will build.\n            - None or -1 means there is no limit.\n            - `max_input` means the limit is set as largest input tensor.\n            - A positive integer is taken as an explicit limit on the number of elements.\n\n            The default is None. Note that imposing a limit can make contractions\n            exponentially slower to perform.\n\n        backend: Which library to use to perform the required ``tensordot``, ``transpose``\n            and ``einsum`` calls. Should match the types of arrays supplied, See\n            `contract_expression` for generating expressions which convert\n            numpy arrays to and from the backend library automatically.\n\n    Returns:\n        The result of the einsum expression.\n\n    Notes:\n        This function should produce a result identical to that of NumPy's einsum\n        function. The primary difference is ``contract`` will attempt to form\n        intermediates which reduce the overall scaling of the given einsum contraction.\n        By default the worst intermediate formed will be equal to that of the largest\n        input array. For large einsum expressions with many input arrays this can\n        provide arbitrarily large (1000 fold+) speed improvements.\n\n        For contractions with just two tensors this function will attempt to use\n        NumPy's built-in BLAS functionality to ensure that the given operation is\n        performed optimally. When NumPy is linked to a threaded BLAS, potential\n        speedups are on the order of 20-100 for a six core machine.\n    \"\"\"\n    if optimize is True:\n        optimize = \"auto\"\n\n    operands_list = [subscripts] + list(operands)\n    einsum_kwargs = {\"out\": out, \"dtype\": dtype, \"order\": order, \"casting\": casting}\n\n    # If no optimization, run pure einsum\n    if optimize is False:\n        return _einsum(*operands_list, **einsum_kwargs)\n\n    # Grab non-einsum kwargs\n    gen_expression = kwargs.pop(\"_gen_expression\", False)\n    constants_dict = kwargs.pop(\"_constants_dict\", {})\n    if len(kwargs):\n        raise TypeError(f\"Did not understand the following kwargs: {kwargs.keys()}\")\n\n    if gen_expression:\n        full_str = operands_list[0]\n\n    # Build the contraction list and operand\n    contraction_list: ContractionListType\n    operands, contraction_list = contract_path(  # type: ignore\n        *operands_list, optimize=optimize, memory_limit=memory_limit, einsum_call=True, use_blas=use_blas\n    )\n\n    # check if performing contraction or just building expression\n    if gen_expression:\n        return ContractExpression(full_str, contraction_list, constants_dict, dtype=dtype, order=order, casting=casting)\n\n    return _core_contract(\n        operands, contraction_list, backend=backend, out=out, dtype=dtype, order=order, casting=casting\n    )\n</code></pre>"},{"location":"api_reference/#opt_einsumcontract_path","title":"<code>opt_einsum.contract_path</code>","text":"<p>Find a contraction order <code>path</code>, without performing the contraction.</p> <p>Parameters:</p> Name Type Description Default <code>subscripts</code> <code>Any</code> <p>Specifies the subscripts for summation.</p> required <code>*operands</code> <code>Any</code> <p>These are the arrays for the operation.</p> <code>()</code> <code>use_blas</code> <code>bool</code> <p>Do you use BLAS for valid operations, may use extra memory for more intermediates.</p> <code>True</code> <code>optimize</code> <code>OptimizeKind</code> <p>Choose the type of path the contraction will be optimized with. - if a list is given uses this as the path. - <code>'optimal'</code> An algorithm that explores all possible ways of contracting the listed tensors. Scales factorially with the number of terms in the contraction. - <code>'dp'</code> A faster (but essentially optimal) algorithm that uses dynamic programming to exhaustively search all contraction paths without outer-products. - <code>'greedy'</code> An cheap algorithm that heuristically chooses the best pairwise contraction at each step. Scales linearly in the number of terms in the contraction. - <code>'random-greedy'</code> Run a randomized version of the greedy algorithm 32 times and pick the best path. - <code>'random-greedy-128'</code> Run a randomized version of the greedy algorithm 128 times and pick the best path. - <code>'branch-all'</code> An algorithm like optimal but that restricts itself to searching 'likely' paths. Still scales factorially. - <code>'branch-2'</code> An even more restricted version of 'branch-all' that only searches the best two options at each step. Scales exponentially with the number of terms in the contraction. - <code>'auto'</code> Choose the best of the above algorithms whilst aiming to keep the path finding time below 1ms. - <code>'auto-hq'</code> Aim for a high quality contraction, choosing the best of the above algorithms whilst aiming to keep the path finding time below 1sec.</p> <code>True</code> <code>memory_limit</code> <code>_MemoryLimit</code> <p>Give the upper bound of the largest intermediate tensor contract will build. - None or -1 means there is no limit - <code>max_input</code> means the limit is set as largest input tensor - a positive integer is taken as an explicit limit on the number of elements</p> <p>The default is None. Note that imposing a limit can make contractions exponentially slower to perform.</p> <code>None</code> <code>shapes</code> <code>bool</code> <p>Whether <code>contract_path</code> should assume arrays (the default) or array shapes have been supplied.</p> <code>False</code> <p>Returns:       path: The optimized einsum contraciton path       PathInfo: A printable object containing various information about the path found.</p> <p>Notes:       The resulting path indicates which terms of the input contraction should be       contracted first, the result of this contraction is then appended to the end of       the contraction list.</p> <p>Examples:       We can begin with a chain dot example. In this case, it is optimal to       contract the b and c tensors represented by the first element of the path (1,       2). The resulting tensor is added to the end of the contraction and the       remaining contraction, <code>(0, 1)</code>, is then executed.</p> <pre><code>a = np.random.rand(2, 2)\nb = np.random.rand(2, 5)\nc = np.random.rand(5, 2)\npath_info = opt_einsum.contract_path('ij,jk,kl-&gt;il', a, b, c)\nprint(path_info[0])\n#&gt; [(1, 2), (0, 1)]\nprint(path_info[1])\n#&gt;   Complete contraction:  ij,jk,kl-&gt;il\n#&gt;          Naive scaling:  4\n#&gt;      Optimized scaling:  3\n#&gt;       Naive FLOP count:  1.600e+02\n#&gt;   Optimized FLOP count:  5.600e+01\n#&gt;    Theoretical speedup:  2.857\n#&gt;   Largest intermediate:  4.000e+00 elements\n#&gt; -------------------------------------------------------------------------\n#&gt; scaling                  current                                remaining\n#&gt; -------------------------------------------------------------------------\n#&gt;    3                   kl,jk-&gt;jl                                ij,jl-&gt;il\n#&gt;    3                   jl,ij-&gt;il                                   il-&gt;il\n</code></pre> <p>A more complex index transformation example.</p> <pre><code>I = np.random.rand(10, 10, 10, 10)\nC = np.random.rand(10, 10)\npath_info = oe.contract_path('ea,fb,abcd,gc,hd-&gt;efgh', C, C, I, C, C)\n\nprint(path_info[0])\n#&gt; [(0, 2), (0, 3), (0, 2), (0, 1)]\nprint(path_info[1])\n#&gt;   Complete contraction:  ea,fb,abcd,gc,hd-&gt;efgh\n#&gt;          Naive scaling:  8\n#&gt;      Optimized scaling:  5\n#&gt;       Naive FLOP count:  8.000e+08\n#&gt;   Optimized FLOP count:  8.000e+05\n#&gt;    Theoretical speedup:  1000.000\n#&gt;   Largest intermediate:  1.000e+04 elements\n#&gt; --------------------------------------------------------------------------\n#&gt; scaling                  current                                remaining\n#&gt; --------------------------------------------------------------------------\n#&gt;    5               abcd,ea-&gt;bcde                      fb,gc,hd,bcde-&gt;efgh\n#&gt;    5               bcde,fb-&gt;cdef                         gc,hd,cdef-&gt;efgh\n#&gt;    5               cdef,gc-&gt;defg                            hd,defg-&gt;efgh\n#&gt;    5               defg,hd-&gt;efgh                               efgh-&gt;efgh\n</code></pre> Source code in <code>opt_einsum/contract.py</code> <pre><code>def contract_path(\n    subscripts: Any,\n    *operands: Any,\n    use_blas: bool = True,\n    optimize: OptimizeKind = True,\n    memory_limit: _MemoryLimit = None,\n    shapes: bool = False,\n    **kwargs: Any,\n) -&gt; Tuple[PathType, PathInfo]:\n    \"\"\"\n      Find a contraction order `path`, without performing the contraction.\n\n    Parameters:\n          subscripts: Specifies the subscripts for summation.\n          *operands: These are the arrays for the operation.\n          use_blas: Do you use BLAS for valid operations, may use extra memory for more intermediates.\n          optimize: Choose the type of path the contraction will be optimized with.\n                - if a list is given uses this as the path.\n                - `'optimal'` An algorithm that explores all possible ways of\n                contracting the listed tensors. Scales factorially with the number of\n                terms in the contraction.\n                - `'dp'` A faster (but essentially optimal) algorithm that uses\n                dynamic programming to exhaustively search all contraction paths\n                without outer-products.\n                - `'greedy'` An cheap algorithm that heuristically chooses the best\n                pairwise contraction at each step. Scales linearly in the number of\n                terms in the contraction.\n                - `'random-greedy'` Run a randomized version of the greedy algorithm\n                32 times and pick the best path.\n                - `'random-greedy-128'` Run a randomized version of the greedy\n                algorithm 128 times and pick the best path.\n                - `'branch-all'` An algorithm like optimal but that restricts itself\n                to searching 'likely' paths. Still scales factorially.\n                - `'branch-2'` An even more restricted version of 'branch-all' that\n                only searches the best two options at each step. Scales exponentially\n                with the number of terms in the contraction.\n                - `'auto'` Choose the best of the above algorithms whilst aiming to\n                keep the path finding time below 1ms.\n                - `'auto-hq'` Aim for a high quality contraction, choosing the best\n                of the above algorithms whilst aiming to keep the path finding time\n                below 1sec.\n\n          memory_limit: Give the upper bound of the largest intermediate tensor contract will build.\n                - None or -1 means there is no limit\n                - `max_input` means the limit is set as largest input tensor\n                - a positive integer is taken as an explicit limit on the number of elements\n\n                The default is None. Note that imposing a limit can make contractions\n                exponentially slower to perform.\n\n          shapes: Whether ``contract_path`` should assume arrays (the default) or array shapes have been supplied.\n\n      Returns:\n          path: The optimized einsum contraciton path\n          PathInfo: A printable object containing various information about the path found.\n\n      Notes:\n          The resulting path indicates which terms of the input contraction should be\n          contracted first, the result of this contraction is then appended to the end of\n          the contraction list.\n\n      Examples:\n          We can begin with a chain dot example. In this case, it is optimal to\n          contract the b and c tensors represented by the first element of the path (1,\n          2). The resulting tensor is added to the end of the contraction and the\n          remaining contraction, `(0, 1)`, is then executed.\n\n      ```python\n      a = np.random.rand(2, 2)\n      b = np.random.rand(2, 5)\n      c = np.random.rand(5, 2)\n      path_info = opt_einsum.contract_path('ij,jk,kl-&gt;il', a, b, c)\n      print(path_info[0])\n      #&gt; [(1, 2), (0, 1)]\n      print(path_info[1])\n      #&gt;   Complete contraction:  ij,jk,kl-&gt;il\n      #&gt;          Naive scaling:  4\n      #&gt;      Optimized scaling:  3\n      #&gt;       Naive FLOP count:  1.600e+02\n      #&gt;   Optimized FLOP count:  5.600e+01\n      #&gt;    Theoretical speedup:  2.857\n      #&gt;   Largest intermediate:  4.000e+00 elements\n      #&gt; -------------------------------------------------------------------------\n      #&gt; scaling                  current                                remaining\n      #&gt; -------------------------------------------------------------------------\n      #&gt;    3                   kl,jk-&gt;jl                                ij,jl-&gt;il\n      #&gt;    3                   jl,ij-&gt;il                                   il-&gt;il\n      ```\n\n      A more complex index transformation example.\n\n      ```python\n      I = np.random.rand(10, 10, 10, 10)\n      C = np.random.rand(10, 10)\n      path_info = oe.contract_path('ea,fb,abcd,gc,hd-&gt;efgh', C, C, I, C, C)\n\n      print(path_info[0])\n      #&gt; [(0, 2), (0, 3), (0, 2), (0, 1)]\n      print(path_info[1])\n      #&gt;   Complete contraction:  ea,fb,abcd,gc,hd-&gt;efgh\n      #&gt;          Naive scaling:  8\n      #&gt;      Optimized scaling:  5\n      #&gt;       Naive FLOP count:  8.000e+08\n      #&gt;   Optimized FLOP count:  8.000e+05\n      #&gt;    Theoretical speedup:  1000.000\n      #&gt;   Largest intermediate:  1.000e+04 elements\n      #&gt; --------------------------------------------------------------------------\n      #&gt; scaling                  current                                remaining\n      #&gt; --------------------------------------------------------------------------\n      #&gt;    5               abcd,ea-&gt;bcde                      fb,gc,hd,bcde-&gt;efgh\n      #&gt;    5               bcde,fb-&gt;cdef                         gc,hd,cdef-&gt;efgh\n      #&gt;    5               cdef,gc-&gt;defg                            hd,defg-&gt;efgh\n      #&gt;    5               defg,hd-&gt;efgh                               efgh-&gt;efgh\n      ```\n    \"\"\"\n    if optimize is True:\n        optimize = \"auto\"\n\n    # Hidden option, only einsum should call this\n    einsum_call_arg = kwargs.pop(\"einsum_call\", False)\n    if len(kwargs):\n        raise TypeError(f\"Did not understand the following kwargs: {kwargs.keys()}\")\n\n    # Python side parsing\n    operands_ = [subscripts] + list(operands)\n    input_subscripts, output_subscript, operands_prepped = parser.parse_einsum_input(operands_, shapes=shapes)\n\n    # Build a few useful list and sets\n    input_list = input_subscripts.split(\",\")\n    input_sets = [frozenset(x) for x in input_list]\n    if shapes:\n        input_shapes = operands_prepped\n    else:\n        input_shapes = [x.shape for x in operands_prepped]\n    output_set = frozenset(output_subscript)\n    indices = frozenset(input_subscripts.replace(\",\", \"\"))\n\n    # Get length of each unique dimension and ensure all dimensions are correct\n    size_dict: Dict[str, int] = {}\n    for tnum, term in enumerate(input_list):\n        sh = input_shapes[tnum]\n\n        if len(sh) != len(term):\n            raise ValueError(\n                \"Einstein sum subscript '{}' does not contain the \"\n                \"correct number of indices for operand {}.\".format(input_list[tnum], tnum)\n            )\n        for cnum, char in enumerate(term):\n            dim = int(sh[cnum])\n\n            if char in size_dict:\n                # For broadcasting cases we always want the largest dim size\n                if size_dict[char] == 1:\n                    size_dict[char] = dim\n                elif dim not in (1, size_dict[char]):\n                    raise ValueError(\n                        \"Size of label '{}' for operand {} ({}) does not match previous \"\n                        \"terms ({}).\".format(char, tnum, size_dict[char], dim)\n                    )\n            else:\n                size_dict[char] = dim\n\n    # Compute size of each input array plus the output array\n    size_list = [helpers.compute_size_by_dict(term, size_dict) for term in input_list + [output_subscript]]\n    memory_arg = _choose_memory_arg(memory_limit, size_list)\n\n    num_ops = len(input_list)\n\n    # Compute naive cost\n    # This is not quite right, need to look into exactly how einsum does this\n    # indices_in_input = input_subscripts.replace(',', '')\n    inner_product = (sum(len(x) for x in input_sets) - len(indices)) &gt; 0\n    naive_cost = helpers.flop_count(indices, inner_product, num_ops, size_dict)\n\n    # Compute the path\n    if not isinstance(optimize, (str, paths.PathOptimizer)):\n        # Custom path supplied\n        path_tuple: PathType = optimize  # type: ignore\n    elif num_ops &lt;= 2:\n        # Nothing to be optimized\n        path_tuple = [tuple(range(num_ops))]\n    elif isinstance(optimize, paths.PathOptimizer):\n        # Custom path optimizer supplied\n        path_tuple = optimize(input_sets, output_set, size_dict, memory_arg)\n    else:\n        path_optimizer = paths.get_path_fn(optimize)\n        path_tuple = path_optimizer(input_sets, output_set, size_dict, memory_arg)\n\n    cost_list = []\n    scale_list = []\n    size_list = []\n    contraction_list = []\n\n    # Build contraction tuple (positions, gemm, einsum_str, remaining)\n    for cnum, contract_inds in enumerate(path_tuple):\n        # Make sure we remove inds from right to left\n        contract_inds = tuple(sorted(list(contract_inds), reverse=True))\n\n        contract_tuple = helpers.find_contraction(contract_inds, input_sets, output_set)\n        out_inds, input_sets, idx_removed, idx_contract = contract_tuple\n\n        # Compute cost, scale, and size\n        cost = helpers.flop_count(idx_contract, bool(idx_removed), len(contract_inds), size_dict)\n        cost_list.append(cost)\n        scale_list.append(len(idx_contract))\n        size_list.append(helpers.compute_size_by_dict(out_inds, size_dict))\n\n        tmp_inputs = [input_list.pop(x) for x in contract_inds]\n        tmp_shapes = [input_shapes.pop(x) for x in contract_inds]\n\n        if use_blas:\n            do_blas = blas.can_blas(tmp_inputs, \"\".join(out_inds), idx_removed, tmp_shapes)\n        else:\n            do_blas = False\n\n        # Last contraction\n        if (cnum - len(path_tuple)) == -1:\n            idx_result = output_subscript\n        else:\n            # use tensordot order to minimize transpositions\n            all_input_inds = \"\".join(tmp_inputs)\n            idx_result = \"\".join(sorted(out_inds, key=all_input_inds.find))\n\n        shp_result = parser.find_output_shape(tmp_inputs, tmp_shapes, idx_result)\n\n        input_list.append(idx_result)\n        input_shapes.append(shp_result)\n\n        einsum_str = \",\".join(tmp_inputs) + \"-&gt;\" + idx_result\n\n        # for large expressions saving the remaining terms at each step can\n        # incur a large memory footprint - and also be messy to print\n        if len(input_list) &lt;= 20:\n            remaining: Optional[Tuple[str, ...]] = tuple(input_list)\n        else:\n            remaining = None\n\n        contraction = (contract_inds, idx_removed, einsum_str, remaining, do_blas)\n        contraction_list.append(contraction)\n\n    opt_cost = sum(cost_list)\n\n    if einsum_call_arg:\n        return operands_prepped, contraction_list  # type: ignore\n\n    path_print = PathInfo(\n        contraction_list,\n        input_subscripts,\n        output_subscript,\n        indices,\n        path_tuple,\n        scale_list,\n        naive_cost,\n        opt_cost,\n        size_list,\n        size_dict,\n    )\n\n    return path_tuple, path_print\n</code></pre>"},{"location":"api_reference/#opt_einsumcontract_expression","title":"<code>opt_einsum.contract_expression</code>","text":"<p>Generate a reusable expression for a given contraction with specific shapes, which can, for example, be cached.</p> <p>Parameters:</p> <pre><code>subscripts: Specifies the subscripts for summation.\nshapes: Shapes of the arrays to optimize the contraction for.\nconstants: The indices of any constant arguments in `shapes`, in which case the\n    actual array should be supplied at that position rather than just a\n    shape. If these are specified, then constant parts of the contraction\n    between calls will be reused. Additionally, if a GPU-enabled backend is\n    used for example, then the constant tensors will be kept on the GPU,\n    minimizing transfers.\nkwargs: Passed on to `contract_path` or `einsum`. See `contract`.\n</code></pre> <p>Returns:</p> Type Description <code>ContractExpression</code> <p>Callable with signature <code>expr(*arrays, out=None, backend='numpy')</code> where the array's shapes should match <code>shapes</code>.</p> <p>Notes:</p> <pre><code>The `out` keyword argument should be supplied to the generated expression\nrather than this function.\nThe `backend` keyword argument should also be supplied to the generated\nexpression. If numpy arrays are supplied, if possible they will be\nconverted to and back from the correct backend array type.\nThe generated expression will work with any arrays which have\nthe same rank (number of dimensions) as the original shapes, however, if\nthe actual sizes are different, the expression may no longer be optimal.\nConstant operations will be computed upon the first call with a particular\nbackend, then subsequently reused.\n</code></pre> <p>Examples:</p> <p>Basic usage:</p> <pre><code>expr = contract_expression(\"ab,bc-&gt;ac\", (3, 4), (4, 5))\na, b = np.random.rand(3, 4), np.random.rand(4, 5)\nc = expr(a, b)\nnp.allclose(c, a @ b)\n#&gt; True\n</code></pre> <p>Supply <code>a</code> as a constant:</p> <pre><code>expr = contract_expression(\"ab,bc-&gt;ac\", a, (4, 5), constants=[0])\nexpr\n#&gt; &lt;ContractExpression('[ab],bc-&gt;ac', constants=[0])&gt;\n\nc = expr(b)\nnp.allclose(c, a @ b)\n#&gt; True\n</code></pre> Source code in <code>opt_einsum/contract.py</code> <pre><code>def contract_expression(\n    subscripts: Union[str, ArrayType, TensorShapeType],\n    *shapes: Union[ArrayType, TensorShapeType, Collection[int]],\n    constants: Union[Collection[int], None] = None,\n    use_blas: bool = True,\n    optimize: OptimizeKind = True,\n    memory_limit: _MemoryLimit = None,\n    **kwargs: Any,\n) -&gt; ContractExpression:\n    \"\"\"Generate a reusable expression for a given contraction with\n    specific shapes, which can, for example, be cached.\n\n    Parameters:\n\n        subscripts: Specifies the subscripts for summation.\n        shapes: Shapes of the arrays to optimize the contraction for.\n        constants: The indices of any constant arguments in `shapes`, in which case the\n            actual array should be supplied at that position rather than just a\n            shape. If these are specified, then constant parts of the contraction\n            between calls will be reused. Additionally, if a GPU-enabled backend is\n            used for example, then the constant tensors will be kept on the GPU,\n            minimizing transfers.\n        kwargs: Passed on to `contract_path` or `einsum`. See `contract`.\n\n    Returns:\n        Callable with signature `expr(*arrays, out=None, backend='numpy')` where the array's shapes should match `shapes`.\n\n    Notes:\n\n        The `out` keyword argument should be supplied to the generated expression\n        rather than this function.\n        The `backend` keyword argument should also be supplied to the generated\n        expression. If numpy arrays are supplied, if possible they will be\n        converted to and back from the correct backend array type.\n        The generated expression will work with any arrays which have\n        the same rank (number of dimensions) as the original shapes, however, if\n        the actual sizes are different, the expression may no longer be optimal.\n        Constant operations will be computed upon the first call with a particular\n        backend, then subsequently reused.\n\n    Examples:\n\n    Basic usage:\n\n    ```python\n    expr = contract_expression(\"ab,bc-&gt;ac\", (3, 4), (4, 5))\n    a, b = np.random.rand(3, 4), np.random.rand(4, 5)\n    c = expr(a, b)\n    np.allclose(c, a @ b)\n    #&gt; True\n    ```\n\n    Supply `a` as a constant:\n\n    ```python\n    expr = contract_expression(\"ab,bc-&gt;ac\", a, (4, 5), constants=[0])\n    expr\n    #&gt; &lt;ContractExpression('[ab],bc-&gt;ac', constants=[0])&gt;\n\n    c = expr(b)\n    np.allclose(c, a @ b)\n    #&gt; True\n    ```\n\n    \"\"\"\n    if not optimize:\n        raise ValueError(\"Can only generate expressions for optimized contractions.\")\n\n    for arg in (\"out\", \"backend\"):\n        if kwargs.get(arg, None) is not None:\n            raise ValueError(\n                \"'{}' should only be specified when calling a \"\n                \"`ContractExpression`, not when building it.\".format(arg)\n            )\n\n    if not isinstance(subscripts, str):\n        subscripts, shapes = parser.convert_interleaved_input((subscripts,) + shapes)  # type: ignore\n\n    kwargs[\"_gen_expression\"] = True\n\n    # build dict of constant indices mapped to arrays\n    constants = constants or tuple()\n    constants_dict = {i: shapes[i] for i in constants}\n    kwargs[\"_constants_dict\"] = constants_dict\n\n    # apart from constant arguments, make dummy arrays\n    dummy_arrays = [s if i in constants else shape_only(s) for i, s in enumerate(shapes)]  # type: ignore\n\n    return contract(\n        subscripts, *dummy_arrays, use_blas=use_blas, optimize=optimize, memory_limit=memory_limit, **kwargs\n    )\n</code></pre>"},{"location":"api_reference/#opt_einsumcontractcontractexpression","title":"<code>opt_einsum.contract.ContractExpression</code>","text":"<p>Helper class for storing an explicit <code>contraction_list</code> which can then be repeatedly called solely with the array arguments.</p> Source code in <code>opt_einsum/contract.py</code> <pre><code>class ContractExpression:\n    \"\"\"Helper class for storing an explicit ``contraction_list`` which can\n    then be repeatedly called solely with the array arguments.\n    \"\"\"\n\n    def __init__(\n        self,\n        contraction: str,\n        contraction_list: ContractionListType,\n        constants_dict: Dict[int, ArrayType],\n        dtype: Optional[str] = None,\n        order: _OrderKACF = \"K\",\n        casting: _Casting = \"safe\",\n    ):\n        self.contraction_list = contraction_list\n        self.dtype = dtype\n        self.order = order\n        self.casting = casting\n        self.contraction = format_const_einsum_str(contraction, constants_dict.keys())\n\n        # need to know _full_num_args to parse constants with, and num_args to call with\n        self._full_num_args = contraction.count(\",\") + 1\n        self.num_args = self._full_num_args - len(constants_dict)\n\n        # likewise need to know full contraction list\n        self._full_contraction_list = contraction_list\n\n        self._constants_dict = constants_dict\n        self._evaluated_constants: Dict[str, Any] = {}\n        self._backend_expressions: Dict[str, Any] = {}\n\n    def evaluate_constants(self, backend: Optional[str] = \"auto\") -&gt; None:\n        \"\"\"Convert any constant operands to the correct backend form, and\n        perform as many contractions as possible to create a new list of\n        operands, stored in ``self._evaluated_constants[backend]``. This also\n        makes sure ``self.contraction_list`` only contains the remaining,\n        non-const operations.\n        \"\"\"\n        # prepare a list of operands, with `None` for non-consts\n        tmp_const_ops = [self._constants_dict.get(i, None) for i in range(self._full_num_args)]\n        backend = parse_backend(tmp_const_ops, backend)\n\n        # get the new list of operands with constant operations performed, and remaining contractions\n        try:\n            new_ops, new_contraction_list = backends.evaluate_constants(backend, tmp_const_ops, self)\n        except KeyError:\n            new_ops, new_contraction_list = self(*tmp_const_ops, backend=backend, evaluate_constants=True)\n\n        self._evaluated_constants[backend] = new_ops\n        self.contraction_list = new_contraction_list\n\n    def _get_evaluated_constants(self, backend: str) -&gt; List[Optional[ArrayType]]:\n        \"\"\"Retrieve or generate the cached list of constant operators (mixed\n        in with None representing non-consts) and the remaining contraction\n        list.\n        \"\"\"\n        try:\n            return self._evaluated_constants[backend]\n        except KeyError:\n            self.evaluate_constants(backend)\n            return self._evaluated_constants[backend]\n\n    def _get_backend_expression(self, arrays: Sequence[ArrayType], backend: str) -&gt; Any:\n        try:\n            return self._backend_expressions[backend]\n        except KeyError:\n            fn = backends.build_expression(backend, arrays, self)\n            self._backend_expressions[backend] = fn\n            return fn\n\n    def _contract(\n        self,\n        arrays: Sequence[ArrayType],\n        out: Optional[ArrayType] = None,\n        backend: Optional[str] = \"auto\",\n        evaluate_constants: bool = False,\n    ) -&gt; ArrayType:\n        \"\"\"The normal, core contraction.\"\"\"\n        contraction_list = self._full_contraction_list if evaluate_constants else self.contraction_list\n\n        return _core_contract(\n            list(arrays),\n            contraction_list,\n            out=out,\n            backend=backend,\n            evaluate_constants=evaluate_constants,\n            dtype=self.dtype,\n            order=self.order,\n            casting=self.casting,\n        )\n\n    def _contract_with_conversion(\n        self,\n        arrays: Sequence[ArrayType],\n        out: Optional[ArrayType],\n        backend: str,\n        evaluate_constants: bool = False,\n    ) -&gt; ArrayType:\n        \"\"\"Special contraction, i.e., contraction with a different backend\n        but converting to and from that backend. Retrieves or generates a\n        cached expression using ``arrays`` as templates, then calls it\n        with ``arrays``.\n\n        If ``evaluate_constants=True``, perform a partial contraction that\n        prepares the constant tensors and operations with the right backend.\n        \"\"\"\n        # convert consts to correct type &amp; find reduced contraction list\n        if evaluate_constants:\n            return backends.evaluate_constants(backend, arrays, self)\n\n        result = self._get_backend_expression(arrays, backend)(*arrays)\n\n        if out is not None:\n            out[()] = result\n            return out\n\n        return result\n\n    def __call__(\n        self,\n        *arrays: ArrayType,\n        out: Union[None, ArrayType] = None,\n        backend: str = \"auto\",\n        evaluate_constants: bool = False,\n    ) -&gt; ArrayType:\n        \"\"\"Evaluate this expression with a set of arrays.\n\n        Parameters:\n            arrays: The arrays to supply as input to the expression.\n            out: If specified, output the result into this array.\n            backend: Perform the contraction with this backend library. If numpy arrays\n                are supplied then try to convert them to and from the correct\n                backend array type.\n            evaluate_constants: Pre-evaluates constants with the appropriate backend.\n\n        Returns:\n            The contracted result.\n        \"\"\"\n\n        backend = parse_backend(arrays, backend)\n\n        correct_num_args = self._full_num_args if evaluate_constants else self.num_args\n\n        if len(arrays) != correct_num_args:\n            raise ValueError(\n                \"This `ContractExpression` takes exactly {} array arguments \"\n                \"but received {}.\".format(self.num_args, len(arrays))\n            )\n\n        if self._constants_dict and not evaluate_constants:\n            # fill in the missing non-constant terms with newly supplied arrays\n            ops_var, ops_const = iter(arrays), self._get_evaluated_constants(backend)\n            ops: Sequence[ArrayType] = [next(ops_var) if op is None else op for op in ops_const]\n        else:\n            ops = arrays\n\n        try:\n            # Check if the backend requires special preparation / calling\n            #   but also ignore non-numpy arrays -&gt; assume user wants same type back\n            if backends.has_backend(backend) and all(infer_backend(x) == \"numpy\" for x in arrays):\n                return self._contract_with_conversion(ops, out, backend, evaluate_constants=evaluate_constants)\n\n            return self._contract(ops, out=out, backend=backend, evaluate_constants=evaluate_constants)\n\n        except ValueError as err:\n            original_msg = str(err.args) if err.args else \"\"\n            msg = (\n                \"Internal error while evaluating `ContractExpression`. Note that few checks are performed\"\n                \" - the number and rank of the array arguments must match the original expression. \"\n                \"The internal error was: '{}'\".format(original_msg),\n            )\n            err.args = msg\n            raise\n\n    def __repr__(self) -&gt; str:\n        if self._constants_dict:\n            constants_repr = \", constants={}\".format(sorted(self._constants_dict))\n        else:\n            constants_repr = \"\"\n        return \"&lt;ContractExpression('{}'{})&gt;\".format(self.contraction, constants_repr)\n\n    def __str__(self) -&gt; str:\n        s = [self.__repr__()]\n        for i, c in enumerate(self.contraction_list):\n            s.append(\"\\n  {}.  \".format(i + 1))\n            s.append(\"'{}'\".format(c[2]) + (\" [{}]\".format(c[-1]) if c[-1] else \"\"))\n            kwargs = {\"dtype\": self.dtype, \"order\": self.order, \"casting\": self.casting}\n            s.append(f\"\\neinsum_kwargs={kwargs}\")\n        return \"\".join(s)\n</code></pre>"},{"location":"api_reference/#opt_einsum.contract.ContractExpression.__call__","title":"<code>__call__(*arrays, out=None, backend='auto', evaluate_constants=False)</code>","text":"<p>Evaluate this expression with a set of arrays.</p> <p>Parameters:</p> Name Type Description Default <code>arrays</code> <code>ArrayType</code> <p>The arrays to supply as input to the expression.</p> <code>()</code> <code>out</code> <code>Union[None, ArrayType]</code> <p>If specified, output the result into this array.</p> <code>None</code> <code>backend</code> <code>str</code> <p>Perform the contraction with this backend library. If numpy arrays are supplied then try to convert them to and from the correct backend array type.</p> <code>'auto'</code> <code>evaluate_constants</code> <code>bool</code> <p>Pre-evaluates constants with the appropriate backend.</p> <code>False</code> <p>Returns:</p> Type Description <code>ArrayType</code> <p>The contracted result.</p> Source code in <code>opt_einsum/contract.py</code> <pre><code>def __call__(\n    self,\n    *arrays: ArrayType,\n    out: Union[None, ArrayType] = None,\n    backend: str = \"auto\",\n    evaluate_constants: bool = False,\n) -&gt; ArrayType:\n    \"\"\"Evaluate this expression with a set of arrays.\n\n    Parameters:\n        arrays: The arrays to supply as input to the expression.\n        out: If specified, output the result into this array.\n        backend: Perform the contraction with this backend library. If numpy arrays\n            are supplied then try to convert them to and from the correct\n            backend array type.\n        evaluate_constants: Pre-evaluates constants with the appropriate backend.\n\n    Returns:\n        The contracted result.\n    \"\"\"\n\n    backend = parse_backend(arrays, backend)\n\n    correct_num_args = self._full_num_args if evaluate_constants else self.num_args\n\n    if len(arrays) != correct_num_args:\n        raise ValueError(\n            \"This `ContractExpression` takes exactly {} array arguments \"\n            \"but received {}.\".format(self.num_args, len(arrays))\n        )\n\n    if self._constants_dict and not evaluate_constants:\n        # fill in the missing non-constant terms with newly supplied arrays\n        ops_var, ops_const = iter(arrays), self._get_evaluated_constants(backend)\n        ops: Sequence[ArrayType] = [next(ops_var) if op is None else op for op in ops_const]\n    else:\n        ops = arrays\n\n    try:\n        # Check if the backend requires special preparation / calling\n        #   but also ignore non-numpy arrays -&gt; assume user wants same type back\n        if backends.has_backend(backend) and all(infer_backend(x) == \"numpy\" for x in arrays):\n            return self._contract_with_conversion(ops, out, backend, evaluate_constants=evaluate_constants)\n\n        return self._contract(ops, out=out, backend=backend, evaluate_constants=evaluate_constants)\n\n    except ValueError as err:\n        original_msg = str(err.args) if err.args else \"\"\n        msg = (\n            \"Internal error while evaluating `ContractExpression`. Note that few checks are performed\"\n            \" - the number and rank of the array arguments must match the original expression. \"\n            \"The internal error was: '{}'\".format(original_msg),\n        )\n        err.args = msg\n        raise\n</code></pre>"},{"location":"api_reference/#opt_einsum.contract.ContractExpression.evaluate_constants","title":"<code>evaluate_constants(backend='auto')</code>","text":"<p>Convert any constant operands to the correct backend form, and perform as many contractions as possible to create a new list of operands, stored in <code>self._evaluated_constants[backend]</code>. This also makes sure <code>self.contraction_list</code> only contains the remaining, non-const operations.</p> Source code in <code>opt_einsum/contract.py</code> <pre><code>def evaluate_constants(self, backend: Optional[str] = \"auto\") -&gt; None:\n    \"\"\"Convert any constant operands to the correct backend form, and\n    perform as many contractions as possible to create a new list of\n    operands, stored in ``self._evaluated_constants[backend]``. This also\n    makes sure ``self.contraction_list`` only contains the remaining,\n    non-const operations.\n    \"\"\"\n    # prepare a list of operands, with `None` for non-consts\n    tmp_const_ops = [self._constants_dict.get(i, None) for i in range(self._full_num_args)]\n    backend = parse_backend(tmp_const_ops, backend)\n\n    # get the new list of operands with constant operations performed, and remaining contractions\n    try:\n        new_ops, new_contraction_list = backends.evaluate_constants(backend, tmp_const_ops, self)\n    except KeyError:\n        new_ops, new_contraction_list = self(*tmp_const_ops, backend=backend, evaluate_constants=True)\n\n    self._evaluated_constants[backend] = new_ops\n    self.contraction_list = new_contraction_list\n</code></pre>"},{"location":"api_reference/#opt_einsumcontractpathinfo","title":"<code>opt_einsum.contract.PathInfo</code>","text":"<p>A printable object to contain information about a contraction path.</p> Source code in <code>opt_einsum/contract.py</code> <pre><code>class PathInfo:\n    \"\"\"A printable object to contain information about a contraction path.\"\"\"\n\n    def __init__(\n        self,\n        contraction_list: ContractionListType,\n        input_subscripts: str,\n        output_subscript: str,\n        indices: ArrayIndexType,\n        path: PathType,\n        scale_list: Sequence[int],\n        naive_cost: int,\n        opt_cost: int,\n        size_list: Sequence[int],\n        size_dict: Dict[str, int],\n    ):\n        self.contraction_list = contraction_list\n        self.input_subscripts = input_subscripts\n        self.output_subscript = output_subscript\n        self.path = path\n        self.indices = indices\n        self.scale_list = scale_list\n        self.naive_cost = Decimal(naive_cost)\n        self.opt_cost = Decimal(opt_cost)\n        self.speedup = self.naive_cost / max(self.opt_cost, Decimal(1))\n        self.size_list = size_list\n        self.size_dict = size_dict\n\n        self.shapes = [tuple(size_dict[k] for k in ks) for ks in input_subscripts.split(\",\")]\n        self.eq = \"{}-&gt;{}\".format(input_subscripts, output_subscript)\n        self.largest_intermediate = Decimal(max(size_list, default=1))\n\n    def __repr__(self) -&gt; str:\n        # Return the path along with a nice string representation\n        header = (\"scaling\", \"BLAS\", \"current\", \"remaining\")\n\n        path_print = [\n            \"  Complete contraction:  {}\\n\".format(self.eq),\n            \"         Naive scaling:  {}\\n\".format(len(self.indices)),\n            \"     Optimized scaling:  {}\\n\".format(max(self.scale_list, default=0)),\n            \"      Naive FLOP count:  {:.3e}\\n\".format(self.naive_cost),\n            \"  Optimized FLOP count:  {:.3e}\\n\".format(self.opt_cost),\n            \"   Theoretical speedup:  {:.3e}\\n\".format(self.speedup),\n            \"  Largest intermediate:  {:.3e} elements\\n\".format(self.largest_intermediate),\n            \"-\" * 80 + \"\\n\",\n            \"{:&gt;6} {:&gt;11} {:&gt;22} {:&gt;37}\\n\".format(*header),\n            \"-\" * 80,\n        ]\n\n        for n, contraction in enumerate(self.contraction_list):\n            _, _, einsum_str, remaining, do_blas = contraction\n\n            if remaining is not None:\n                remaining_str = \",\".join(remaining) + \"-&gt;\" + self.output_subscript\n            else:\n                remaining_str = \"...\"\n            size_remaining = max(0, 56 - max(22, len(einsum_str)))\n\n            path_run = (\n                self.scale_list[n],\n                do_blas,\n                einsum_str,\n                remaining_str,\n                size_remaining,\n            )\n            path_print.append(\"\\n{:&gt;4} {:&gt;14} {:&gt;22}    {:&gt;{}}\".format(*path_run))\n\n        return \"\".join(path_print)\n</code></pre>"},{"location":"api_reference/#opt_einsumget_symbol","title":"<code>opt_einsum.get_symbol</code>","text":"<p>Get the symbol corresponding to int <code>i</code> - runs through the usual 52 letters before resorting to unicode characters, starting at <code>chr(192)</code> and skipping surrogates.</p> <p>Examples:</p> <pre><code>get_symbol(2)\n#&gt; 'c'\n\nget_symbol(200)\n#&gt; '\u0154'\n\nget_symbol(20000)\n#&gt; '\u4eac'\n</code></pre> Source code in <code>opt_einsum/parser.py</code> <pre><code>def get_symbol(i: int) -&gt; str:\n    \"\"\"Get the symbol corresponding to int ``i`` - runs through the usual 52\n    letters before resorting to unicode characters, starting at ``chr(192)`` and skipping surrogates.\n\n    **Examples:**\n\n    ```python\n    get_symbol(2)\n    #&gt; 'c'\n\n    get_symbol(200)\n    #&gt; '\u0154'\n\n    get_symbol(20000)\n    #&gt; '\u4eac'\n    ```\n    \"\"\"\n    if i &lt; 52:\n        return _einsum_symbols_base[i]\n    elif i &gt;= 55296:\n        # Skip chr(57343) - chr(55296) as surrogates\n        return chr(i + 2048)\n    else:\n        return chr(i + 140)\n</code></pre>"},{"location":"api_reference/#opt_einsumshared_intermediates","title":"<code>opt_einsum.shared_intermediates</code>","text":"<p>Context in which contract intermediate results are shared.</p> <p>Note that intermediate computations will not be garbage collected until 1. this context exits, and 2. the yielded cache is garbage collected (if it was captured).</p> <p>Parameters:</p> <ul> <li>cache - (dict) If specified, a user-stored dict in which intermediate results will be stored. This can be used to interleave sharing contexts.</li> </ul> <p>Returns:</p> <ul> <li>cache - (dict) A dictionary in which sharing results are stored. If ignored,     sharing results will be garbage collected when this context is     exited. This dict can be passed to another context to resume     sharing.</li> </ul> Source code in <code>opt_einsum/sharing.py</code> <pre><code>@contextlib.contextmanager\ndef shared_intermediates(\n    cache: Optional[CacheType] = None,\n) -&gt; Generator[CacheType, None, None]:\n    \"\"\"Context in which contract intermediate results are shared.\n\n    Note that intermediate computations will not be garbage collected until\n    1. this context exits, and\n    2. the yielded cache is garbage collected (if it was captured).\n\n    **Parameters:**\n\n    - **cache** - *(dict)* If specified, a user-stored dict in which intermediate results will be stored. This can be used to interleave sharing contexts.\n\n    **Returns:**\n\n    - **cache** - *(dict)* A dictionary in which sharing results are stored. If ignored,\n        sharing results will be garbage collected when this context is\n        exited. This dict can be passed to another context to resume\n        sharing.\n    \"\"\"\n    if cache is None:\n        cache = {}\n    _add_sharing_cache(cache)\n    try:\n        yield cache\n    finally:\n        _remove_sharing_cache()\n</code></pre>"},{"location":"api_reference/#opt_einsumpathsoptimal","title":"<code>opt_einsum.paths.optimal</code>","text":"<p>Computes all possible pair contractions in a depth-first recursive manner, sieving results based on <code>memory_limit</code> and the best path found so far.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[ArrayIndexType]</code> <p>List of sets that represent the lhs side of the einsum subscript.</p> required <code>output</code> <code>ArrayIndexType</code> <p>Set that represents the rhs side of the overall einsum subscript.</p> required <code>size_dict</code> <code>Dict[str, int]</code> <p>Dictionary of index sizes.</p> required <code>memory_limit</code> <code>Optional[int]</code> <p>The maximum number of elements in a temporary array.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>path</code> <code>PathType</code> <p>The optimal contraction order within the memory limit constraint.</p> <p>Examples:</p> <pre><code>isets = [set('abd'), set('ac'), set('bdc')]\noset = set('')\nidx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}\noptimal(isets, oset, idx_sizes, 5000)\n#&gt; [(0, 2), (0, 1)]\n</code></pre> Source code in <code>opt_einsum/paths.py</code> <pre><code>def optimal(\n    inputs: List[ArrayIndexType],\n    output: ArrayIndexType,\n    size_dict: Dict[str, int],\n    memory_limit: Optional[int] = None,\n) -&gt; PathType:\n    \"\"\"\n    Computes all possible pair contractions in a depth-first recursive manner,\n    sieving results based on `memory_limit` and the best path found so far.\n\n    Parameters:\n        inputs: List of sets that represent the lhs side of the einsum subscript.\n        output: Set that represents the rhs side of the overall einsum subscript.\n        size_dict: Dictionary of index sizes.\n        memory_limit: The maximum number of elements in a temporary array.\n\n    Returns:\n        path: The optimal contraction order within the memory limit constraint.\n\n    Examples:\n\n    ```python\n    isets = [set('abd'), set('ac'), set('bdc')]\n    oset = set('')\n    idx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}\n    optimal(isets, oset, idx_sizes, 5000)\n    #&gt; [(0, 2), (0, 1)]\n    ```\n    \"\"\"\n    inputs_set = tuple(map(frozenset, inputs))\n    output_set = frozenset(output)\n\n    best_flops = {\"flops\": float(\"inf\")}\n    best_ssa_path = {\"ssa_path\": (tuple(range(len(inputs))),)}\n    size_cache: Dict[FrozenSet[str], int] = {}\n    result_cache: Dict[Tuple[ArrayIndexType, ArrayIndexType], Tuple[FrozenSet[str], int]] = {}\n\n    def _optimal_iterate(path, remaining, inputs, flops):\n\n        # reached end of path (only ever get here if flops is best found so far)\n        if len(remaining) == 1:\n            best_flops[\"flops\"] = flops\n            best_ssa_path[\"ssa_path\"] = path\n            return\n\n        # check all possible remaining paths\n        for i, j in itertools.combinations(remaining, 2):\n            if i &gt; j:\n                i, j = j, i\n            key = (inputs[i], inputs[j])\n            try:\n                k12, flops12 = result_cache[key]\n            except KeyError:\n                k12, flops12 = result_cache[key] = calc_k12_flops(inputs, output_set, remaining, i, j, size_dict)\n\n            # sieve based on current best flops\n            new_flops = flops + flops12\n            if new_flops &gt;= best_flops[\"flops\"]:\n                continue\n\n            # sieve based on memory limit\n            if memory_limit not in _UNLIMITED_MEM:\n                try:\n                    size12 = size_cache[k12]\n                except KeyError:\n                    size12 = size_cache[k12] = compute_size_by_dict(k12, size_dict)\n\n                # possibly terminate this path with an all-terms einsum\n                if size12 &gt; memory_limit:\n                    new_flops = flops + _compute_oversize_flops(inputs, remaining, output_set, size_dict)\n                    if new_flops &lt; best_flops[\"flops\"]:\n                        best_flops[\"flops\"] = new_flops\n                        best_ssa_path[\"ssa_path\"] = path + (tuple(remaining),)\n                    continue\n\n            # add contraction and recurse into all remaining\n            _optimal_iterate(\n                path=path + ((i, j),),\n                inputs=inputs + (k12,),\n                remaining=remaining - {i, j} | {len(inputs)},\n                flops=new_flops,\n            )\n\n    _optimal_iterate(path=(), inputs=inputs_set, remaining=set(range(len(inputs))), flops=0)\n\n    return ssa_to_linear(best_ssa_path[\"ssa_path\"])\n</code></pre>"},{"location":"api_reference/#opt_einsumpathsgreedy","title":"<code>opt_einsum.paths.greedy</code>","text":"<p>Finds the path by a three stage algorithm:</p> <ol> <li>Eagerly compute Hadamard products.</li> <li>Greedily compute contractions to maximize <code>removed_size</code></li> <li>Greedily compute outer products.</li> </ol> <p>This algorithm scales quadratically with respect to the maximum number of elements sharing a common dim.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>List[ArrayIndexType]</code> <p>List of sets that represent the lhs side of the einsum subscript</p> required <code>output</code> <code>ArrayIndexType</code> <p>Set that represents the rhs side of the overall einsum subscript</p> required <code>size_dict</code> <code>Dict[str, int]</code> <p>Dictionary of index sizes</p> required <code>memory_limit</code> <code>Optional[int]</code> <p>The maximum number of elements in a temporary array</p> <code>None</code> <code>choose_fn</code> <code>Any</code> <p>A function that chooses which contraction to perform from the queue</p> <code>None</code> <code>cost_fn</code> <code>str</code> <p>A function that assigns a potential contraction a cost.</p> <code>'memory-removed'</code> <p>Returns:</p> Name Type Description <code>path</code> <code>PathType</code> <p>The contraction order (a list of tuples of ints).</p> <p>Examples:</p> <pre><code>```python\nisets = [set('abd'), set('ac'), set('bdc')]\noset = set('')\nidx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}\ngreedy(isets, oset, idx_sizes)\n#&gt; [(0, 2), (0, 1)]\n```\n</code></pre> Source code in <code>opt_einsum/paths.py</code> <pre><code>def greedy(\n    inputs: List[ArrayIndexType],\n    output: ArrayIndexType,\n    size_dict: Dict[str, int],\n    memory_limit: Optional[int] = None,\n    choose_fn: Any = None,\n    cost_fn: str = \"memory-removed\",\n) -&gt; PathType:\n    \"\"\"\n    Finds the path by a three stage algorithm:\n\n    1. Eagerly compute Hadamard products.\n    2. Greedily compute contractions to maximize `removed_size`\n    3. Greedily compute outer products.\n\n    This algorithm scales quadratically with respect to the\n    maximum number of elements sharing a common dim.\n\n    Parameters:\n        inputs: List of sets that represent the lhs side of the einsum subscript\n        output: Set that represents the rhs side of the overall einsum subscript\n        size_dict: Dictionary of index sizes\n        memory_limit: The maximum number of elements in a temporary array\n        choose_fn: A function that chooses which contraction to perform from the queue\n        cost_fn: A function that assigns a potential contraction a cost.\n\n    Returns:\n        path: The contraction order (a list of tuples of ints).\n\n    Examples:\n\n        ```python\n        isets = [set('abd'), set('ac'), set('bdc')]\n        oset = set('')\n        idx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}\n        greedy(isets, oset, idx_sizes)\n        #&gt; [(0, 2), (0, 1)]\n        ```\n    \"\"\"\n    if memory_limit not in _UNLIMITED_MEM:\n        return branch(inputs, output, size_dict, memory_limit, nbranch=1, cost_fn=cost_fn)  # type: ignore\n\n    ssa_path = ssa_greedy_optimize(inputs, output, size_dict, cost_fn=cost_fn, choose_fn=choose_fn)\n    return ssa_to_linear(ssa_path)\n</code></pre>"},{"location":"api_reference/#opt_einsumpathsbranch","title":"<code>opt_einsum.paths.branch</code>","text":"Source code in <code>opt_einsum/paths.py</code> <pre><code>def branch(\n    inputs: List[ArrayIndexType],\n    output: ArrayIndexType,\n    size_dict: Dict[str, int],\n    memory_limit: Optional[int] = None,\n    **optimizer_kwargs: Dict[str, Any],\n) -&gt; PathType:\n    optimizer = BranchBound(**optimizer_kwargs)  # type: ignore\n    return optimizer(inputs, output, size_dict, memory_limit)\n</code></pre>"},{"location":"api_reference/#opt_einsumpathspathoptimizer","title":"<code>opt_einsum.paths.PathOptimizer</code>","text":"<p>Base class for different path optimizers to inherit from.</p> <p>Subclassed optimizers should define a call method with signature:</p> <pre><code>def __call__(self, inputs, output, size_dict, memory_limit=None):\n    \"\"\"\n    Parameters:\n        inputs : list[set[str]]\n            The indices of each input array.\n        outputs : set[str]\n            The output indices\n        size_dict : dict[str, int]\n            The size of each index\n        memory_limit : int, optional\n            If given, the maximum allowed memory.\n    \"\"\"\n    # ... compute path here ...\n    return path\n</code></pre> <p>where <code>path</code> is a list of int-tuples specifying a contraction order.</p> Source code in <code>opt_einsum/paths.py</code> <pre><code>class PathOptimizer:\n    \"\"\"Base class for different path optimizers to inherit from.\n\n    Subclassed optimizers should define a call method with signature:\n\n    ```python\n    def __call__(self, inputs, output, size_dict, memory_limit=None):\n        \\\"\\\"\\\"\n        Parameters:\n            inputs : list[set[str]]\n                The indices of each input array.\n            outputs : set[str]\n                The output indices\n            size_dict : dict[str, int]\n                The size of each index\n            memory_limit : int, optional\n                If given, the maximum allowed memory.\n        \\\"\\\"\\\"\n        # ... compute path here ...\n        return path\n    ```\n\n    where `path` is a list of int-tuples specifying a contraction order.\n    \"\"\"\n\n    def _check_args_against_first_call(\n        self,\n        inputs: List[ArrayIndexType],\n        output: ArrayIndexType,\n        size_dict: Dict[str, int],\n    ) -&gt; None:\n        \"\"\"Utility that stateful optimizers can use to ensure they are not\n        called with different contractions across separate runs.\n        \"\"\"\n        args = (inputs, output, size_dict)\n        if not hasattr(self, \"_first_call_args\"):\n            # simply set the attribute as currently there is no global PathOptimizer init\n            self._first_call_args = args\n        elif args != self._first_call_args:\n            raise ValueError(\n                \"The arguments specifying the contraction that this path optimizer \"\n                \"instance was called with have changed - try creating a new instance.\"\n            )\n\n    def __call__(\n        self,\n        inputs: List[ArrayIndexType],\n        output: ArrayIndexType,\n        size_dict: Dict[str, int],\n        memory_limit: Optional[int] = None,\n    ) -&gt; PathType:\n        raise NotImplementedError\n</code></pre>"},{"location":"api_reference/#opt_einsumpathsbranchbound","title":"<code>opt_einsum.paths.BranchBound</code>","text":"<p>               Bases: <code>PathOptimizer</code></p> Source code in <code>opt_einsum/paths.py</code> <pre><code>class BranchBound(PathOptimizer):\n    def __init__(\n        self,\n        nbranch: Optional[int] = None,\n        cutoff_flops_factor: int = 4,\n        minimize: str = \"flops\",\n        cost_fn: str = \"memory-removed\",\n    ):\n        \"\"\"\n        Explores possible pair contractions in a depth-first recursive manner like\n        the `optimal` approach, but with extra heuristic early pruning of branches\n        as well sieving by `memory_limit` and the best path found so far.\n\n\n        Parameters:\n            nbranch: How many branches to explore at each contraction step. If None, explore\n                all possible branches. If an integer, branch into this many paths at\n                each step. Defaults to None.\n            cutoff_flops_factor: If at any point, a path is doing this much worse than the best path\n                found so far was, terminate it. The larger this is made, the more paths\n                will be fully explored and the slower the algorithm. Defaults to 4.\n            minimize: Whether to optimize the path with regard primarily to the total\n                estimated flop-count, or the size of the largest intermediate. The\n                option not chosen will still be used as a secondary criterion.\n            cost_fn: A function that returns a heuristic 'cost' of a potential contraction\n                with which to sort candidates. Should have signature\n                `cost_fn(size12, size1, size2, k12, k1, k2)`.\n        \"\"\"\n        if (nbranch is not None) and nbranch &lt; 1:\n            raise ValueError(f\"The number of branches must be at least one, `nbranch={nbranch}`.\")\n\n        self.nbranch = nbranch\n        self.cutoff_flops_factor = cutoff_flops_factor\n        self.minimize = minimize\n        self.cost_fn: Any = _COST_FNS.get(cost_fn, cost_fn)\n\n        self.better = get_better_fn(minimize)\n        self.best: Dict[str, Any] = {\"flops\": float(\"inf\"), \"size\": float(\"inf\")}\n        self.best_progress: Dict[int, float] = defaultdict(lambda: float(\"inf\"))\n\n    @property\n    def path(self) -&gt; PathType:\n        return ssa_to_linear(self.best[\"ssa_path\"])\n\n    def __call__(\n        self,\n        inputs_: List[ArrayIndexType],\n        output_: ArrayIndexType,\n        size_dict: Dict[str, int],\n        memory_limit: Optional[int] = None,\n    ) -&gt; PathType:\n        \"\"\"\n\n        Parameters:\n            inputs_: List of sets that represent the lhs side of the einsum subscript\n            output_: Set that represents the rhs side of the overall einsum subscript\n            size_dict: Dictionary of index sizes\n            memory_limit: The maximum number of elements in a temporary array\n\n        Returns:\n            path: The contraction order within the memory limit constraint.\n\n        Examples:\n\n        ```python\n        isets = [set('abd'), set('ac'), set('bdc')]\n        oset = set('')\n        idx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}\n        optimal(isets, oset, idx_sizes, 5000)\n        #&gt; [(0, 2), (0, 1)]\n        \"\"\"\n        self._check_args_against_first_call(inputs_, output_, size_dict)\n\n        inputs: Tuple[FrozenSet[str]] = tuple(map(frozenset, inputs_))  # type: ignore\n        output: FrozenSet[str] = frozenset(output_)\n\n        size_cache = {k: compute_size_by_dict(k, size_dict) for k in inputs}\n        result_cache: Dict[Tuple[FrozenSet[str], FrozenSet[str]], Tuple[FrozenSet[str], int]] = {}\n\n        def _branch_iterate(path, inputs, remaining, flops, size):\n\n            # reached end of path (only ever get here if flops is best found so far)\n            if len(remaining) == 1:\n                self.best[\"size\"] = size\n                self.best[\"flops\"] = flops\n                self.best[\"ssa_path\"] = path\n                return\n\n            def _assess_candidate(k1: FrozenSet[str], k2: FrozenSet[str], i: int, j: int) -&gt; Any:\n                # find resulting indices and flops\n                try:\n                    k12, flops12 = result_cache[k1, k2]\n                except KeyError:\n                    k12, flops12 = result_cache[k1, k2] = calc_k12_flops(inputs, output, remaining, i, j, size_dict)\n\n                try:\n                    size12 = size_cache[k12]\n                except KeyError:\n                    size12 = size_cache[k12] = compute_size_by_dict(k12, size_dict)\n\n                new_flops = flops + flops12\n                new_size = max(size, size12)\n\n                # sieve based on current best i.e. check flops and size still better\n                if not self.better(new_flops, new_size, self.best[\"flops\"], self.best[\"size\"]):\n                    return None\n\n                # compare to how the best method was doing as this point\n                if new_flops &lt; self.best_progress[len(inputs)]:\n                    self.best_progress[len(inputs)] = new_flops\n                # sieve based on current progress relative to best\n                elif new_flops &gt; self.cutoff_flops_factor * self.best_progress[len(inputs)]:\n                    return None\n\n                # sieve based on memory limit\n                if (memory_limit not in _UNLIMITED_MEM) and (size12 &gt; memory_limit):  # type: ignore\n                    # terminate path here, but check all-terms contract first\n                    new_flops = flops + _compute_oversize_flops(inputs, remaining, output_, size_dict)\n                    if new_flops &lt; self.best[\"flops\"]:\n                        self.best[\"flops\"] = new_flops\n                        self.best[\"ssa_path\"] = path + (tuple(remaining),)\n                    return None\n\n                # set cost heuristic in order to locally sort possible contractions\n                size1, size2 = size_cache[inputs[i]], size_cache[inputs[j]]\n                cost = self.cost_fn(size12, size1, size2, k12, k1, k2)\n\n                return cost, flops12, new_flops, new_size, (i, j), k12\n\n            # check all possible remaining paths\n            candidates = []\n            for i, j in itertools.combinations(remaining, 2):\n                if i &gt; j:\n                    i, j = j, i\n                k1, k2 = inputs[i], inputs[j]\n\n                # initially ignore outer products\n                if k1.isdisjoint(k2):\n                    continue\n\n                candidate = _assess_candidate(k1, k2, i, j)\n                if candidate:\n                    heapq.heappush(candidates, candidate)\n\n            # assess outer products if nothing left\n            if not candidates:\n                for i, j in itertools.combinations(remaining, 2):\n                    if i &gt; j:\n                        i, j = j, i\n                    k1, k2 = inputs[i], inputs[j]\n                    candidate = _assess_candidate(k1, k2, i, j)\n                    if candidate:\n                        heapq.heappush(candidates, candidate)\n\n            # recurse into all or some of the best candidate contractions\n            bi = 0\n            while (self.nbranch is None or bi &lt; self.nbranch) and candidates:\n                _, _, new_flops, new_size, (i, j), k12 = heapq.heappop(candidates)\n                _branch_iterate(\n                    path=path + ((i, j),),\n                    inputs=inputs + (k12,),\n                    remaining=(remaining - {i, j}) | {len(inputs)},\n                    flops=new_flops,\n                    size=new_size,\n                )\n                bi += 1\n\n        _branch_iterate(path=(), inputs=inputs, remaining=set(range(len(inputs))), flops=0, size=0)\n\n        return self.path\n</code></pre>"},{"location":"api_reference/#opt_einsum.paths.BranchBound.__call__","title":"<code>__call__(inputs_, output_, size_dict, memory_limit=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>inputs_</code> <code>List[ArrayIndexType]</code> <p>List of sets that represent the lhs side of the einsum subscript</p> required <code>output_</code> <code>ArrayIndexType</code> <p>Set that represents the rhs side of the overall einsum subscript</p> required <code>size_dict</code> <code>Dict[str, int]</code> <p>Dictionary of index sizes</p> required <code>memory_limit</code> <code>Optional[int]</code> <p>The maximum number of elements in a temporary array</p> <code>None</code> <p>Returns:</p> Name Type Description <code>path</code> <code>PathType</code> <p>The contraction order within the memory limit constraint.</p> <p>Examples:</p> <p>```python isets = [set('abd'), set('ac'), set('bdc')] oset = set('') idx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4} optimal(isets, oset, idx_sizes, 5000)</p>"},{"location":"api_reference/#opt_einsum.paths.BranchBound.__call__--0-2-0-1","title":"&gt; [(0, 2), (0, 1)]","text":"Source code in <code>opt_einsum/paths.py</code> <pre><code>def __call__(\n    self,\n    inputs_: List[ArrayIndexType],\n    output_: ArrayIndexType,\n    size_dict: Dict[str, int],\n    memory_limit: Optional[int] = None,\n) -&gt; PathType:\n    \"\"\"\n\n    Parameters:\n        inputs_: List of sets that represent the lhs side of the einsum subscript\n        output_: Set that represents the rhs side of the overall einsum subscript\n        size_dict: Dictionary of index sizes\n        memory_limit: The maximum number of elements in a temporary array\n\n    Returns:\n        path: The contraction order within the memory limit constraint.\n\n    Examples:\n\n    ```python\n    isets = [set('abd'), set('ac'), set('bdc')]\n    oset = set('')\n    idx_sizes = {'a': 1, 'b':2, 'c':3, 'd':4}\n    optimal(isets, oset, idx_sizes, 5000)\n    #&gt; [(0, 2), (0, 1)]\n    \"\"\"\n    self._check_args_against_first_call(inputs_, output_, size_dict)\n\n    inputs: Tuple[FrozenSet[str]] = tuple(map(frozenset, inputs_))  # type: ignore\n    output: FrozenSet[str] = frozenset(output_)\n\n    size_cache = {k: compute_size_by_dict(k, size_dict) for k in inputs}\n    result_cache: Dict[Tuple[FrozenSet[str], FrozenSet[str]], Tuple[FrozenSet[str], int]] = {}\n\n    def _branch_iterate(path, inputs, remaining, flops, size):\n\n        # reached end of path (only ever get here if flops is best found so far)\n        if len(remaining) == 1:\n            self.best[\"size\"] = size\n            self.best[\"flops\"] = flops\n            self.best[\"ssa_path\"] = path\n            return\n\n        def _assess_candidate(k1: FrozenSet[str], k2: FrozenSet[str], i: int, j: int) -&gt; Any:\n            # find resulting indices and flops\n            try:\n                k12, flops12 = result_cache[k1, k2]\n            except KeyError:\n                k12, flops12 = result_cache[k1, k2] = calc_k12_flops(inputs, output, remaining, i, j, size_dict)\n\n            try:\n                size12 = size_cache[k12]\n            except KeyError:\n                size12 = size_cache[k12] = compute_size_by_dict(k12, size_dict)\n\n            new_flops = flops + flops12\n            new_size = max(size, size12)\n\n            # sieve based on current best i.e. check flops and size still better\n            if not self.better(new_flops, new_size, self.best[\"flops\"], self.best[\"size\"]):\n                return None\n\n            # compare to how the best method was doing as this point\n            if new_flops &lt; self.best_progress[len(inputs)]:\n                self.best_progress[len(inputs)] = new_flops\n            # sieve based on current progress relative to best\n            elif new_flops &gt; self.cutoff_flops_factor * self.best_progress[len(inputs)]:\n                return None\n\n            # sieve based on memory limit\n            if (memory_limit not in _UNLIMITED_MEM) and (size12 &gt; memory_limit):  # type: ignore\n                # terminate path here, but check all-terms contract first\n                new_flops = flops + _compute_oversize_flops(inputs, remaining, output_, size_dict)\n                if new_flops &lt; self.best[\"flops\"]:\n                    self.best[\"flops\"] = new_flops\n                    self.best[\"ssa_path\"] = path + (tuple(remaining),)\n                return None\n\n            # set cost heuristic in order to locally sort possible contractions\n            size1, size2 = size_cache[inputs[i]], size_cache[inputs[j]]\n            cost = self.cost_fn(size12, size1, size2, k12, k1, k2)\n\n            return cost, flops12, new_flops, new_size, (i, j), k12\n\n        # check all possible remaining paths\n        candidates = []\n        for i, j in itertools.combinations(remaining, 2):\n            if i &gt; j:\n                i, j = j, i\n            k1, k2 = inputs[i], inputs[j]\n\n            # initially ignore outer products\n            if k1.isdisjoint(k2):\n                continue\n\n            candidate = _assess_candidate(k1, k2, i, j)\n            if candidate:\n                heapq.heappush(candidates, candidate)\n\n        # assess outer products if nothing left\n        if not candidates:\n            for i, j in itertools.combinations(remaining, 2):\n                if i &gt; j:\n                    i, j = j, i\n                k1, k2 = inputs[i], inputs[j]\n                candidate = _assess_candidate(k1, k2, i, j)\n                if candidate:\n                    heapq.heappush(candidates, candidate)\n\n        # recurse into all or some of the best candidate contractions\n        bi = 0\n        while (self.nbranch is None or bi &lt; self.nbranch) and candidates:\n            _, _, new_flops, new_size, (i, j), k12 = heapq.heappop(candidates)\n            _branch_iterate(\n                path=path + ((i, j),),\n                inputs=inputs + (k12,),\n                remaining=(remaining - {i, j}) | {len(inputs)},\n                flops=new_flops,\n                size=new_size,\n            )\n            bi += 1\n\n    _branch_iterate(path=(), inputs=inputs, remaining=set(range(len(inputs))), flops=0, size=0)\n\n    return self.path\n</code></pre>"},{"location":"api_reference/#opt_einsum.paths.BranchBound.__init__","title":"<code>__init__(nbranch=None, cutoff_flops_factor=4, minimize='flops', cost_fn='memory-removed')</code>","text":"<p>Explores possible pair contractions in a depth-first recursive manner like the <code>optimal</code> approach, but with extra heuristic early pruning of branches as well sieving by <code>memory_limit</code> and the best path found so far.</p> <p>Parameters:</p> Name Type Description Default <code>nbranch</code> <code>Optional[int]</code> <p>How many branches to explore at each contraction step. If None, explore all possible branches. If an integer, branch into this many paths at each step. Defaults to None.</p> <code>None</code> <code>cutoff_flops_factor</code> <code>int</code> <p>If at any point, a path is doing this much worse than the best path found so far was, terminate it. The larger this is made, the more paths will be fully explored and the slower the algorithm. Defaults to 4.</p> <code>4</code> <code>minimize</code> <code>str</code> <p>Whether to optimize the path with regard primarily to the total estimated flop-count, or the size of the largest intermediate. The option not chosen will still be used as a secondary criterion.</p> <code>'flops'</code> <code>cost_fn</code> <code>str</code> <p>A function that returns a heuristic 'cost' of a potential contraction with which to sort candidates. Should have signature <code>cost_fn(size12, size1, size2, k12, k1, k2)</code>.</p> <code>'memory-removed'</code> Source code in <code>opt_einsum/paths.py</code> <pre><code>def __init__(\n    self,\n    nbranch: Optional[int] = None,\n    cutoff_flops_factor: int = 4,\n    minimize: str = \"flops\",\n    cost_fn: str = \"memory-removed\",\n):\n    \"\"\"\n    Explores possible pair contractions in a depth-first recursive manner like\n    the `optimal` approach, but with extra heuristic early pruning of branches\n    as well sieving by `memory_limit` and the best path found so far.\n\n\n    Parameters:\n        nbranch: How many branches to explore at each contraction step. If None, explore\n            all possible branches. If an integer, branch into this many paths at\n            each step. Defaults to None.\n        cutoff_flops_factor: If at any point, a path is doing this much worse than the best path\n            found so far was, terminate it. The larger this is made, the more paths\n            will be fully explored and the slower the algorithm. Defaults to 4.\n        minimize: Whether to optimize the path with regard primarily to the total\n            estimated flop-count, or the size of the largest intermediate. The\n            option not chosen will still be used as a secondary criterion.\n        cost_fn: A function that returns a heuristic 'cost' of a potential contraction\n            with which to sort candidates. Should have signature\n            `cost_fn(size12, size1, size2, k12, k1, k2)`.\n    \"\"\"\n    if (nbranch is not None) and nbranch &lt; 1:\n        raise ValueError(f\"The number of branches must be at least one, `nbranch={nbranch}`.\")\n\n    self.nbranch = nbranch\n    self.cutoff_flops_factor = cutoff_flops_factor\n    self.minimize = minimize\n    self.cost_fn: Any = _COST_FNS.get(cost_fn, cost_fn)\n\n    self.better = get_better_fn(minimize)\n    self.best: Dict[str, Any] = {\"flops\": float(\"inf\"), \"size\": float(\"inf\")}\n    self.best_progress: Dict[int, float] = defaultdict(lambda: float(\"inf\"))\n</code></pre>"},{"location":"api_reference/#opt_einsumpath_randomrandomoptimizer","title":"<code>opt_einsum.path_random.RandomOptimizer</code>","text":"<p>               Bases: <code>PathOptimizer</code></p> <p>Base class for running any random path finder that benefits from repeated calling, possibly in a parallel fashion. Custom random optimizers should subclass this, and the <code>setup</code> method should be implemented with the following signature:</p> <pre><code>def setup(self, inputs, output, size_dict):\n    # custom preparation here ...\n    return trial_fn, trial_args\n</code></pre> <p>Where <code>trial_fn</code> itself should have the signature::</p> <pre><code>def trial_fn(r, *trial_args):\n    # custom computation of path here\n    return ssa_path, cost, size\n</code></pre> <p>Where <code>r</code> is the run number and could for example be used to seed a random number generator. See <code>RandomGreedy</code> for an example.</p> <p>Parameters:</p> Name Type Description Default <code>max_repeats</code> <code>int</code> <p>The maximum number of repeat trials to have.</p> <code>32</code> <code>max_time</code> <code>Optional[float]</code> <p>The maximum amount of time to run the algorithm for.</p> <code>None</code> <code>minimize</code> <code>str</code> <p>Whether to favour paths that minimize the total estimated flop-count or the size of the largest intermediate created.</p> <code>'flops'</code> <code>parallel</code> <code>Union[bool, Decimal, int]</code> <p>Whether to parallelize the random trials, by default <code>False</code>. If <code>True</code>, use a <code>concurrent.futures.ProcessPoolExecutor</code> with the same number of processes as cores. If an integer is specified, use that many processes instead. Finally, you can supply a custom executor-pool which should have an API matching that of the python 3 standard library module <code>concurrent.futures</code>. Namely, a <code>submit</code> method that returns <code>Future</code> objects, themselves with <code>result</code> and <code>cancel</code> methods.</p> <code>False</code> <code>pre_dispatch</code> <code>int</code> <p>If running in parallel, how many jobs to pre-dispatch so as to avoid submitting all jobs at once. Should also be more than twice the number of workers to avoid under-subscription. Default: 128.</p> <code>128</code> <p>Attributes:</p> Name Type Description <code>path</code> <code>PathType</code> <p>The best path found so far.</p> <code>costs</code> <code>List[int]</code> <p>The list of each trial's costs found so far.</p> <code>sizes</code> <code>List[int]</code> <p>The list of each trial's largest intermediate size so far.</p> Source code in <code>opt_einsum/path_random.py</code> <pre><code>class RandomOptimizer(paths.PathOptimizer):\n    \"\"\"Base class for running any random path finder that benefits\n    from repeated calling, possibly in a parallel fashion. Custom random\n    optimizers should subclass this, and the `setup` method should be\n    implemented with the following signature:\n\n    ```python\n    def setup(self, inputs, output, size_dict):\n        # custom preparation here ...\n        return trial_fn, trial_args\n    ```\n\n    Where `trial_fn` itself should have the signature::\n\n    ```python\n    def trial_fn(r, *trial_args):\n        # custom computation of path here\n        return ssa_path, cost, size\n    ```\n\n    Where `r` is the run number and could for example be used to seed a\n    random number generator. See `RandomGreedy` for an example.\n\n\n    Parameters:\n        max_repeats: The maximum number of repeat trials to have.\n        max_time: The maximum amount of time to run the algorithm for.\n        minimize:  Whether to favour paths that minimize the total estimated flop-count or\n            the size of the largest intermediate created.\n        parallel: Whether to parallelize the random trials, by default `False`. If\n            `True`, use a `concurrent.futures.ProcessPoolExecutor` with the same\n            number of processes as cores. If an integer is specified, use that many\n            processes instead. Finally, you can supply a custom executor-pool which\n            should have an API matching that of the python 3 standard library\n            module `concurrent.futures`. Namely, a `submit` method that returns\n            `Future` objects, themselves with `result` and `cancel` methods.\n        pre_dispatch: If running in parallel, how many jobs to pre-dispatch so as to avoid\n            submitting all jobs at once. Should also be more than twice the number\n            of workers to avoid under-subscription. Default: 128.\n\n    Attributes:\n        path: The best path found so far.\n        costs: The list of each trial's costs found so far.\n        sizes: The list of each trial's largest intermediate size so far.\n    \"\"\"\n\n    def __init__(\n        self,\n        max_repeats: int = 32,\n        max_time: Optional[float] = None,\n        minimize: str = \"flops\",\n        parallel: Union[bool, Decimal, int] = False,\n        pre_dispatch: int = 128,\n    ):\n\n        if minimize not in (\"flops\", \"size\"):\n            raise ValueError(\"`minimize` should be one of {'flops', 'size'}.\")\n\n        self.max_repeats = max_repeats\n        self.max_time = max_time\n        self.minimize = minimize\n        self.better = paths.get_better_fn(minimize)\n        self._parallel: Union[bool, Decimal, int] = False\n        self.parallel = parallel\n        self.pre_dispatch = pre_dispatch\n\n        self.costs: List[int] = []\n        self.sizes: List[int] = []\n        self.best: Dict[str, Any] = {\"flops\": float(\"inf\"), \"size\": float(\"inf\")}\n\n        self._repeats_start = 0\n        self._executor: Any\n        self._futures: Any\n\n    @property\n    def path(self) -&gt; PathType:\n        \"\"\"The best path found so far.\"\"\"\n        return paths.ssa_to_linear(self.best[\"ssa_path\"])\n\n    @property\n    def parallel(self) -&gt; Union[bool, Decimal, int]:\n        return self._parallel\n\n    @parallel.setter\n    def parallel(self, parallel: Union[bool, Decimal, int]) -&gt; None:\n        # shutdown any previous executor if we are managing it\n        if getattr(self, \"_managing_executor\", False):\n            self._executor.shutdown()\n\n        self._parallel = parallel\n        self._managing_executor = False\n\n        if parallel is False:\n            self._executor = None\n            return\n\n        if parallel is True:\n            from concurrent.futures import ProcessPoolExecutor\n\n            self._executor = ProcessPoolExecutor()\n            self._managing_executor = True\n            return\n\n        if isinstance(parallel, (int, Decimal)):\n            from concurrent.futures import ProcessPoolExecutor\n\n            self._executor = ProcessPoolExecutor(int(parallel))\n            self._managing_executor = True\n            return\n\n        # assume a pool-executor has been supplied\n        self._executor = parallel\n\n    def _gen_results_parallel(self, repeats: Iterable[int], trial_fn: Any, args: Any) -&gt; Generator[Any, None, None]:\n        \"\"\"Lazily generate results from an executor without submitting all jobs at once.\"\"\"\n        self._futures = deque()\n\n        # the idea here is to submit at least ``pre_dispatch`` jobs *before* we\n        # yield any results, then do both in tandem, before draining the queue\n        for r in repeats:\n            if len(self._futures) &lt; self.pre_dispatch:\n                self._futures.append(self._executor.submit(trial_fn, r, *args))\n                continue\n            yield self._futures.popleft().result()\n\n        while self._futures:\n            yield self._futures.popleft().result()\n\n    def _cancel_futures(self) -&gt; None:\n        if self._executor is not None:\n            for f in self._futures:\n                f.cancel()\n\n    def setup(\n        self,\n        inputs: List[ArrayIndexType],\n        output: ArrayIndexType,\n        size_dict: Dict[str, int],\n    ) -&gt; Tuple[Any, Any]:\n        raise NotImplementedError\n\n    def __call__(\n        self,\n        inputs: List[ArrayIndexType],\n        output: ArrayIndexType,\n        size_dict: Dict[str, int],\n        memory_limit: Optional[int] = None,\n    ) -&gt; PathType:\n        self._check_args_against_first_call(inputs, output, size_dict)\n\n        # start a timer?\n        if self.max_time is not None:\n            t0 = time.time()\n\n        trial_fn, trial_args = self.setup(inputs, output, size_dict)\n\n        r_start = self._repeats_start + len(self.costs)\n        r_stop = r_start + self.max_repeats\n        repeats = range(r_start, r_stop)\n\n        # create the trials lazily\n        if self._executor is not None:\n            trials = self._gen_results_parallel(repeats, trial_fn, trial_args)\n        else:\n            trials = (trial_fn(r, *trial_args) for r in repeats)\n\n        # assess the trials\n        for ssa_path, cost, size in trials:\n\n            # keep track of all costs and sizes\n            self.costs.append(cost)\n            self.sizes.append(size)\n\n            # check if we have found a new best\n            found_new_best = self.better(cost, size, self.best[\"flops\"], self.best[\"size\"])\n\n            if found_new_best:\n                self.best[\"flops\"] = cost\n                self.best[\"size\"] = size\n                self.best[\"ssa_path\"] = ssa_path\n\n            # check if we have run out of time\n            if (self.max_time is not None) and (time.time() &gt; t0 + self.max_time):\n                break\n\n        self._cancel_futures()\n        return self.path\n\n    def __del__(self):\n        # if we created the parallel pool-executor, shut it down\n        if getattr(self, \"_managing_executor\", False):\n            self._executor.shutdown()\n</code></pre>"},{"location":"api_reference/#opt_einsum.path_random.RandomOptimizer.path","title":"<code>path: PathType</code>  <code>property</code>","text":"<p>The best path found so far.</p>"},{"location":"api_reference/#opt_einsumpath_randomrandomgreedy","title":"<code>opt_einsum.path_random.RandomGreedy</code>","text":"<p>               Bases: <code>RandomOptimizer</code></p> Source code in <code>opt_einsum/path_random.py</code> <pre><code>class RandomGreedy(RandomOptimizer):\n\n    def __init__(\n        self,\n        cost_fn: str = \"memory-removed-jitter\",\n        temperature: float = 1.0,\n        rel_temperature: bool = True,\n        nbranch: int = 8,\n        **kwargs: Any,\n    ):\n        \"\"\"\n        Parameters:\n            cost_fn: A function that returns a heuristic 'cost' of a potential contraction\n                    with which to sort candidates. Should have signature\n                    `cost_fn(size12, size1, size2, k12, k1, k2)`.\n            temperature: When choosing a possible contraction, its relative probability will be\n                    proportional to `exp(-cost / temperature)`. Thus the larger\n                    `temperature` is, the further random paths will stray from the normal\n                    'greedy' path. Conversely, if set to zero, only paths with exactly the\n                    same cost as the best at each step will be explored.\n            rel_temperature: Whether to normalize the ``temperature`` at each step to the scale of\n                    the best cost. This is generally beneficial as the magnitude of costs\n                    can vary significantly throughout a contraction. If False, the\n                    algorithm will end up branching when the absolute cost is low, but\n                    stick to the 'greedy' path when the cost is high - this can also be\n                    beneficial.\n            nbranch: How many potential paths to calculate probability for and choose from at each step.\n            kwargs: Supplied to RandomOptimizer.\n        \"\"\"\n        self.cost_fn = cost_fn\n        self.temperature = temperature\n        self.rel_temperature = rel_temperature\n        self.nbranch = nbranch\n        super().__init__(**kwargs)\n\n    @property\n    def choose_fn(self) -&gt; Any:\n        \"\"\"The function that chooses which contraction to take - make this a\n        property so that ``temperature`` and ``nbranch`` etc. can be updated\n        between runs.\n        \"\"\"\n        if self.nbranch == 1:\n            return None\n\n        return functools.partial(\n            thermal_chooser,\n            temperature=self.temperature,\n            nbranch=self.nbranch,\n            rel_temperature=self.rel_temperature,\n        )\n\n    def setup(\n        self,\n        inputs: List[ArrayIndexType],\n        output: ArrayIndexType,\n        size_dict: Dict[str, int],\n    ) -&gt; Tuple[Any, Any]:\n        fn = _trial_greedy_ssa_path_and_cost\n        args = (inputs, output, size_dict, self.choose_fn, self.cost_fn)\n        return fn, args\n</code></pre>"},{"location":"api_reference/#opt_einsum.path_random.RandomGreedy.choose_fn","title":"<code>choose_fn: Any</code>  <code>property</code>","text":"<p>The function that chooses which contraction to take - make this a property so that <code>temperature</code> and <code>nbranch</code> etc. can be updated between runs.</p>"},{"location":"api_reference/#opt_einsum.path_random.RandomGreedy.__init__","title":"<code>__init__(cost_fn='memory-removed-jitter', temperature=1.0, rel_temperature=True, nbranch=8, **kwargs)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>cost_fn</code> <code>str</code> <p>A function that returns a heuristic 'cost' of a potential contraction     with which to sort candidates. Should have signature     <code>cost_fn(size12, size1, size2, k12, k1, k2)</code>.</p> <code>'memory-removed-jitter'</code> <code>temperature</code> <code>float</code> <p>When choosing a possible contraction, its relative probability will be     proportional to <code>exp(-cost / temperature)</code>. Thus the larger     <code>temperature</code> is, the further random paths will stray from the normal     'greedy' path. Conversely, if set to zero, only paths with exactly the     same cost as the best at each step will be explored.</p> <code>1.0</code> <code>rel_temperature</code> <code>bool</code> <p>Whether to normalize the <code>temperature</code> at each step to the scale of     the best cost. This is generally beneficial as the magnitude of costs     can vary significantly throughout a contraction. If False, the     algorithm will end up branching when the absolute cost is low, but     stick to the 'greedy' path when the cost is high - this can also be     beneficial.</p> <code>True</code> <code>nbranch</code> <code>int</code> <p>How many potential paths to calculate probability for and choose from at each step.</p> <code>8</code> <code>kwargs</code> <code>Any</code> <p>Supplied to RandomOptimizer.</p> <code>{}</code> Source code in <code>opt_einsum/path_random.py</code> <pre><code>def __init__(\n    self,\n    cost_fn: str = \"memory-removed-jitter\",\n    temperature: float = 1.0,\n    rel_temperature: bool = True,\n    nbranch: int = 8,\n    **kwargs: Any,\n):\n    \"\"\"\n    Parameters:\n        cost_fn: A function that returns a heuristic 'cost' of a potential contraction\n                with which to sort candidates. Should have signature\n                `cost_fn(size12, size1, size2, k12, k1, k2)`.\n        temperature: When choosing a possible contraction, its relative probability will be\n                proportional to `exp(-cost / temperature)`. Thus the larger\n                `temperature` is, the further random paths will stray from the normal\n                'greedy' path. Conversely, if set to zero, only paths with exactly the\n                same cost as the best at each step will be explored.\n        rel_temperature: Whether to normalize the ``temperature`` at each step to the scale of\n                the best cost. This is generally beneficial as the magnitude of costs\n                can vary significantly throughout a contraction. If False, the\n                algorithm will end up branching when the absolute cost is low, but\n                stick to the 'greedy' path when the cost is high - this can also be\n                beneficial.\n        nbranch: How many potential paths to calculate probability for and choose from at each step.\n        kwargs: Supplied to RandomOptimizer.\n    \"\"\"\n    self.cost_fn = cost_fn\n    self.temperature = temperature\n    self.rel_temperature = rel_temperature\n    self.nbranch = nbranch\n    super().__init__(**kwargs)\n</code></pre>"},{"location":"api_reference/#opt_einsumpathsdynamicprogramming","title":"<code>opt_einsum.paths.DynamicProgramming</code>","text":"<p>               Bases: <code>PathOptimizer</code></p> <p>Finds the optimal path of pairwise contractions without intermediate outer products based a dynamic programming approach presented in Phys. Rev. E 90, 033315 (2014) (the corresponding preprint is publicly available at https://arxiv.org/abs/1304.6112). This method is especially well-suited in the area of tensor network states, where it usually outperforms all the other optimization strategies.</p> <p>This algorithm shows exponential scaling with the number of inputs in the worst case scenario (see example below). If the graph to be contracted consists of disconnected subgraphs, the algorithm scales linearly in the number of disconnected subgraphs and only exponentially with the number of inputs per subgraph.</p> <p>Parameters:</p> Name Type Description Default <code>minimize</code> <code>str</code> <p>What to minimize: - 'flops' - minimize the number of flops - 'size' - minimize the size of the largest intermediate - 'write' - minimize the size of all intermediate tensors - 'combo' - minimize <code>flops + alpha * write</code> summed over intermediates, a default ratio of alpha=64 is used, or it can be customized with <code>f'combo-{alpha}'</code> - 'limit' - minimize <code>max(flops, alpha * write)</code> summed over intermediates, a default ratio of alpha=64 is used, or it can be customized with <code>f'limit-{alpha}'</code> - callable - a custom local cost function</p> <code>'flops'</code> <code>cost_cap</code> <code>Union[bool, int]</code> <p>How to implement cost-capping: - True - iteratively increase the cost-cap - False - implement no cost-cap at all - int - use explicit cost cap</p> <code>True</code> <code>search_outer</code> <code>bool</code> <p>In rare circumstances the optimal contraction may involve an outer product, this option allows searching such contractions but may well slow down the path finding considerably on all but very small graphs.</p> <code>False</code> Source code in <code>opt_einsum/paths.py</code> <pre><code>class DynamicProgramming(PathOptimizer):\n    \"\"\"\n    Finds the optimal path of pairwise contractions without intermediate outer\n    products based a dynamic programming approach presented in\n    Phys. Rev. E 90, 033315 (2014) (the corresponding preprint is publicly\n    available at https://arxiv.org/abs/1304.6112). This method is especially\n    well-suited in the area of tensor network states, where it usually\n    outperforms all the other optimization strategies.\n\n    This algorithm shows exponential scaling with the number of inputs\n    in the worst case scenario (see example below). If the graph to be\n    contracted consists of disconnected subgraphs, the algorithm scales\n    linearly in the number of disconnected subgraphs and only exponentially\n    with the number of inputs per subgraph.\n\n    Parameters:\n        minimize: What to minimize:\n            - 'flops' - minimize the number of flops\n            - 'size' - minimize the size of the largest intermediate\n            - 'write' - minimize the size of all intermediate tensors\n            - 'combo' - minimize `flops + alpha * write` summed over intermediates, a default ratio of alpha=64\n            is used, or it can be customized with `f'combo-{alpha}'`\n            - 'limit' - minimize `max(flops, alpha * write)` summed over intermediates, a default ratio of alpha=64\n            is used, or it can be customized with `f'limit-{alpha}'`\n            - callable - a custom local cost function\n\n        cost_cap: How to implement cost-capping:\n            - True - iteratively increase the cost-cap\n            - False - implement no cost-cap at all\n            - int - use explicit cost cap\n\n        search_outer: In rare circumstances the optimal contraction may involve an outer\n            product, this option allows searching such contractions but may well\n            slow down the path finding considerably on all but very small graphs.\n    \"\"\"\n\n    def __init__(self, minimize: str = \"flops\", cost_cap: Union[bool, int] = True, search_outer: bool = False) -&gt; None:\n        self.minimize = minimize\n        self.search_outer = search_outer\n        self.cost_cap = cost_cap\n\n    def __call__(\n        self,\n        inputs_: List[ArrayIndexType],\n        output_: ArrayIndexType,\n        size_dict_: Dict[str, int],\n        memory_limit_: Optional[int] = None,\n    ) -&gt; PathType:\n        \"\"\"\n        Parameters:\n            inputs_: List of sets that represent the lhs side of the einsum subscript\n            output_: Set that represents the rhs side of the overall einsum subscript\n            size_dict_: Dictionary of index sizes\n            memory_limit_: The maximum number of elements in a temporary array\n\n        Returns:\n            path: The contraction order (a list of tuples of ints).\n\n        Examples:\n            ```python\n            n_in = 3  # exponential scaling\n            n_out = 2 # linear scaling\n            s = dict()\n            i_all = []\n            for _ in range(n_out):\n                i = [set() for _ in range(n_in)]\n                for j in range(n_in):\n                    for k in range(j+1, n_in):\n                        c = oe.get_symbol(len(s))\n                        i[j].add(c)\n                        i[k].add(c)\n                        s[c] = 2\n                i_all.extend(i)\n            o = DynamicProgramming()\n            o(i_all, set(), s)\n            #&gt; [(1, 2), (0, 4), (1, 2), (0, 2), (0, 1)]\n            ```\n        \"\"\"\n        _check_contraction, naive_scale = _parse_minimize(self.minimize)\n        _check_outer = (lambda x: True) if self.search_outer else (lambda x: x)\n\n        ind_counts = Counter(itertools.chain(*inputs_, output_))\n        all_inds = tuple(ind_counts)\n\n        # convert all indices to integers (makes set operations ~10 % faster)\n        symbol2int = {c: j for j, c in enumerate(all_inds)}\n        inputs = [frozenset(symbol2int[c] for c in i) for i in inputs_]\n        output = frozenset(symbol2int[c] for c in output_)\n        size_dict_canonical = {symbol2int[c]: v for c, v in size_dict_.items() if c in symbol2int}\n        size_dict = [size_dict_canonical[j] for j in range(len(size_dict_canonical))]\n        naive_cost = naive_scale * len(inputs) * functools.reduce(operator.mul, size_dict, 1)\n\n        inputs, inputs_done, inputs_contractions = _dp_parse_out_single_term_ops(inputs, all_inds, ind_counts)\n\n        if not inputs:\n            # nothing left to do after single axis reductions!\n            return _tree_to_sequence(simple_tree_tuple(inputs_done))\n\n        # a list of all necessary contraction expressions for each of the\n        # disconnected subgraphs and their size\n        subgraph_contractions = inputs_done\n        subgraph_contractions_size = [1] * len(inputs_done)\n\n        if self.search_outer:\n            # optimize everything together if we are considering outer products\n            subgraphs = [frozenset(range(len(inputs)))]\n        else:\n            subgraphs = _find_disconnected_subgraphs(inputs, output)\n\n        # the bitmap set of all tensors is computed as it is needed to\n        # compute set differences: s1 - s2 transforms into\n        # s1 &amp; (all_tensors ^ s2)\n        all_tensors = (1 &lt;&lt; len(inputs)) - 1\n\n        for g in subgraphs:\n\n            # dynamic programming approach to compute x[n] for subgraph g;\n            # x[n][set of n tensors] = (indices, cost, contraction)\n            # the set of n tensors is represented by a bitmap: if bit j is 1,\n            # tensor j is in the set, e.g. 0b100101 = {0,2,5}; set unions\n            # (intersections) can then be computed by bitwise or (and);\n            x: List[Any] = [None] * 2 + [dict() for j in range(len(g) - 1)]\n            x[1] = OrderedDict((1 &lt;&lt; j, (inputs[j], 0, inputs_contractions[j])) for j in g)\n\n            # convert set of tensors g to a bitmap set:\n            bitmap_g = functools.reduce(lambda x, y: x | y, (1 &lt;&lt; j for j in g))\n\n            # try to find contraction with cost &lt;= cost_cap and increase\n            # cost_cap successively if no such contraction is found;\n            # this is a major performance improvement; start with product of\n            # output index dimensions as initial cost_cap\n            subgraph_inds = frozenset.union(*_bitmap_select(bitmap_g, inputs))\n            if self.cost_cap is True:\n                cost_cap = compute_size_by_dict(subgraph_inds &amp; output, size_dict)\n            elif self.cost_cap is False:\n                cost_cap = float(\"inf\")  # type: ignore\n            else:\n                cost_cap = self.cost_cap\n            # set the factor to increase the cost by each iteration (ensure &gt; 1)\n            if len(subgraph_inds) == 0:\n                cost_increment = 2\n            else:\n                cost_increment = max(min(map(size_dict.__getitem__, subgraph_inds)), 2)\n\n            while len(x[-1]) == 0:\n                for n in range(2, len(x[1]) + 1):\n                    xn = x[n]\n\n                    # try to combine solutions from x[m] and x[n-m]\n                    for m in range(1, n // 2 + 1):\n                        for s1, (i1, cost1, contract1) in x[m].items():\n                            for s2, (i2, cost2, contract2) in x[n - m].items():\n\n                                # can only merge if s1 and s2 are disjoint\n                                # and avoid e.g. s1={0}, s2={1} and s1={1}, s2={0}\n                                if (not s1 &amp; s2) and (m != n - m or s1 &lt; s2):\n                                    i1_cut_i2_wo_output = (i1 &amp; i2) - output\n\n                                    # maybe ignore outer products:\n                                    if _check_outer(i1_cut_i2_wo_output):\n\n                                        i1_union_i2 = i1 | i2\n                                        _check_contraction(\n                                            cost1,\n                                            cost2,\n                                            i1_union_i2,\n                                            size_dict,\n                                            cost_cap,\n                                            s1,\n                                            s2,\n                                            xn,\n                                            bitmap_g,\n                                            all_tensors,\n                                            inputs,\n                                            i1_cut_i2_wo_output,\n                                            memory_limit_,\n                                            contract1,\n                                            contract2,\n                                        )\n\n                if (cost_cap &gt; naive_cost) and (len(x[-1]) == 0):\n                    raise RuntimeError(\"No contraction found for given `memory_limit`.\")\n\n                # increase cost cap for next iteration:\n                cost_cap = cost_increment * cost_cap\n\n            i, cost, contraction = list(x[-1].values())[0]\n            subgraph_contractions.append(contraction)\n            subgraph_contractions_size.append(compute_size_by_dict(i, size_dict))\n\n        # sort the subgraph contractions by the size of the subgraphs in\n        # ascending order (will give the cheapest contractions); note that\n        # outer products should be performed pairwise (to use BLAS functions)\n        subgraph_contractions = [\n            subgraph_contractions[j]\n            for j in sorted(\n                range(len(subgraph_contractions_size)),\n                key=subgraph_contractions_size.__getitem__,\n            )\n        ]\n\n        # build the final contraction tree\n        tree = simple_tree_tuple(subgraph_contractions)\n        return _tree_to_sequence(tree)\n</code></pre>"},{"location":"api_reference/#opt_einsum.paths.DynamicProgramming.__call__","title":"<code>__call__(inputs_, output_, size_dict_, memory_limit_=None)</code>","text":"<p>Parameters:</p> Name Type Description Default <code>inputs_</code> <code>List[ArrayIndexType]</code> <p>List of sets that represent the lhs side of the einsum subscript</p> required <code>output_</code> <code>ArrayIndexType</code> <p>Set that represents the rhs side of the overall einsum subscript</p> required <code>size_dict_</code> <code>Dict[str, int]</code> <p>Dictionary of index sizes</p> required <code>memory_limit_</code> <code>Optional[int]</code> <p>The maximum number of elements in a temporary array</p> <code>None</code> <p>Returns:</p> Name Type Description <code>path</code> <code>PathType</code> <p>The contraction order (a list of tuples of ints).</p> <p>Examples:</p> <pre><code>n_in = 3  # exponential scaling\nn_out = 2 # linear scaling\ns = dict()\ni_all = []\nfor _ in range(n_out):\n    i = [set() for _ in range(n_in)]\n    for j in range(n_in):\n        for k in range(j+1, n_in):\n            c = oe.get_symbol(len(s))\n            i[j].add(c)\n            i[k].add(c)\n            s[c] = 2\n    i_all.extend(i)\no = DynamicProgramming()\no(i_all, set(), s)\n#&gt; [(1, 2), (0, 4), (1, 2), (0, 2), (0, 1)]\n</code></pre> Source code in <code>opt_einsum/paths.py</code> <pre><code>def __call__(\n    self,\n    inputs_: List[ArrayIndexType],\n    output_: ArrayIndexType,\n    size_dict_: Dict[str, int],\n    memory_limit_: Optional[int] = None,\n) -&gt; PathType:\n    \"\"\"\n    Parameters:\n        inputs_: List of sets that represent the lhs side of the einsum subscript\n        output_: Set that represents the rhs side of the overall einsum subscript\n        size_dict_: Dictionary of index sizes\n        memory_limit_: The maximum number of elements in a temporary array\n\n    Returns:\n        path: The contraction order (a list of tuples of ints).\n\n    Examples:\n        ```python\n        n_in = 3  # exponential scaling\n        n_out = 2 # linear scaling\n        s = dict()\n        i_all = []\n        for _ in range(n_out):\n            i = [set() for _ in range(n_in)]\n            for j in range(n_in):\n                for k in range(j+1, n_in):\n                    c = oe.get_symbol(len(s))\n                    i[j].add(c)\n                    i[k].add(c)\n                    s[c] = 2\n            i_all.extend(i)\n        o = DynamicProgramming()\n        o(i_all, set(), s)\n        #&gt; [(1, 2), (0, 4), (1, 2), (0, 2), (0, 1)]\n        ```\n    \"\"\"\n    _check_contraction, naive_scale = _parse_minimize(self.minimize)\n    _check_outer = (lambda x: True) if self.search_outer else (lambda x: x)\n\n    ind_counts = Counter(itertools.chain(*inputs_, output_))\n    all_inds = tuple(ind_counts)\n\n    # convert all indices to integers (makes set operations ~10 % faster)\n    symbol2int = {c: j for j, c in enumerate(all_inds)}\n    inputs = [frozenset(symbol2int[c] for c in i) for i in inputs_]\n    output = frozenset(symbol2int[c] for c in output_)\n    size_dict_canonical = {symbol2int[c]: v for c, v in size_dict_.items() if c in symbol2int}\n    size_dict = [size_dict_canonical[j] for j in range(len(size_dict_canonical))]\n    naive_cost = naive_scale * len(inputs) * functools.reduce(operator.mul, size_dict, 1)\n\n    inputs, inputs_done, inputs_contractions = _dp_parse_out_single_term_ops(inputs, all_inds, ind_counts)\n\n    if not inputs:\n        # nothing left to do after single axis reductions!\n        return _tree_to_sequence(simple_tree_tuple(inputs_done))\n\n    # a list of all necessary contraction expressions for each of the\n    # disconnected subgraphs and their size\n    subgraph_contractions = inputs_done\n    subgraph_contractions_size = [1] * len(inputs_done)\n\n    if self.search_outer:\n        # optimize everything together if we are considering outer products\n        subgraphs = [frozenset(range(len(inputs)))]\n    else:\n        subgraphs = _find_disconnected_subgraphs(inputs, output)\n\n    # the bitmap set of all tensors is computed as it is needed to\n    # compute set differences: s1 - s2 transforms into\n    # s1 &amp; (all_tensors ^ s2)\n    all_tensors = (1 &lt;&lt; len(inputs)) - 1\n\n    for g in subgraphs:\n\n        # dynamic programming approach to compute x[n] for subgraph g;\n        # x[n][set of n tensors] = (indices, cost, contraction)\n        # the set of n tensors is represented by a bitmap: if bit j is 1,\n        # tensor j is in the set, e.g. 0b100101 = {0,2,5}; set unions\n        # (intersections) can then be computed by bitwise or (and);\n        x: List[Any] = [None] * 2 + [dict() for j in range(len(g) - 1)]\n        x[1] = OrderedDict((1 &lt;&lt; j, (inputs[j], 0, inputs_contractions[j])) for j in g)\n\n        # convert set of tensors g to a bitmap set:\n        bitmap_g = functools.reduce(lambda x, y: x | y, (1 &lt;&lt; j for j in g))\n\n        # try to find contraction with cost &lt;= cost_cap and increase\n        # cost_cap successively if no such contraction is found;\n        # this is a major performance improvement; start with product of\n        # output index dimensions as initial cost_cap\n        subgraph_inds = frozenset.union(*_bitmap_select(bitmap_g, inputs))\n        if self.cost_cap is True:\n            cost_cap = compute_size_by_dict(subgraph_inds &amp; output, size_dict)\n        elif self.cost_cap is False:\n            cost_cap = float(\"inf\")  # type: ignore\n        else:\n            cost_cap = self.cost_cap\n        # set the factor to increase the cost by each iteration (ensure &gt; 1)\n        if len(subgraph_inds) == 0:\n            cost_increment = 2\n        else:\n            cost_increment = max(min(map(size_dict.__getitem__, subgraph_inds)), 2)\n\n        while len(x[-1]) == 0:\n            for n in range(2, len(x[1]) + 1):\n                xn = x[n]\n\n                # try to combine solutions from x[m] and x[n-m]\n                for m in range(1, n // 2 + 1):\n                    for s1, (i1, cost1, contract1) in x[m].items():\n                        for s2, (i2, cost2, contract2) in x[n - m].items():\n\n                            # can only merge if s1 and s2 are disjoint\n                            # and avoid e.g. s1={0}, s2={1} and s1={1}, s2={0}\n                            if (not s1 &amp; s2) and (m != n - m or s1 &lt; s2):\n                                i1_cut_i2_wo_output = (i1 &amp; i2) - output\n\n                                # maybe ignore outer products:\n                                if _check_outer(i1_cut_i2_wo_output):\n\n                                    i1_union_i2 = i1 | i2\n                                    _check_contraction(\n                                        cost1,\n                                        cost2,\n                                        i1_union_i2,\n                                        size_dict,\n                                        cost_cap,\n                                        s1,\n                                        s2,\n                                        xn,\n                                        bitmap_g,\n                                        all_tensors,\n                                        inputs,\n                                        i1_cut_i2_wo_output,\n                                        memory_limit_,\n                                        contract1,\n                                        contract2,\n                                    )\n\n            if (cost_cap &gt; naive_cost) and (len(x[-1]) == 0):\n                raise RuntimeError(\"No contraction found for given `memory_limit`.\")\n\n            # increase cost cap for next iteration:\n            cost_cap = cost_increment * cost_cap\n\n        i, cost, contraction = list(x[-1].values())[0]\n        subgraph_contractions.append(contraction)\n        subgraph_contractions_size.append(compute_size_by_dict(i, size_dict))\n\n    # sort the subgraph contractions by the size of the subgraphs in\n    # ascending order (will give the cheapest contractions); note that\n    # outer products should be performed pairwise (to use BLAS functions)\n    subgraph_contractions = [\n        subgraph_contractions[j]\n        for j in sorted(\n            range(len(subgraph_contractions_size)),\n            key=subgraph_contractions_size.__getitem__,\n        )\n    ]\n\n    # build the final contraction tree\n    tree = simple_tree_tuple(subgraph_contractions)\n    return _tree_to_sequence(tree)\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#330-2020-07-19","title":"3.3.0 / 2020-07-19","text":"<p>Adds a <code>object</code> backend for optimized contractions on arbitrary Python objects.</p> <p>New Features</p> <ul> <li>#145 Adds a <code>object</code> based backend so that <code>contract(backend='object')</code> can be used on arbitrary objects such as SymPy symbols.</li> </ul> <p>Enhancements</p> <ul> <li>#140 Better error messages when the requested <code>contract</code> backend cannot be found.</li> <li>#141 Adds a check with RandomOptimizers to ensure the objects are not accidentally reused for different contractions.</li> <li>#149 Limits the <code>remaining</code> category for the <code>contract_path</code> output to only show up to 20 tensors to prevent issues with the quadratically scaling memory requirements and the number of print lines for large contractions.</li> </ul>"},{"location":"changelog/#320-2020-03-01","title":"3.2.0 / 2020-03-01","text":"<p>Small fixes for the <code>dp</code> path and support for a new mars backend.</p> <p>New Features</p> <ul> <li>#109 Adds mars backend support.</li> </ul> <p>Enhancements</p> <ul> <li>#110 New <code>auto-hq</code> and <code>'random-greedy-128'</code> paths.</li> <li>#119 Fixes several edge cases in the <code>dp</code> path.</li> </ul> <p>Bug fixes</p> <ul> <li>#127 Fixes an issue where Python 3.6 features are required while Python 3.5 is <code>opt_einsum</code>'s stated minimum version.</li> </ul>"},{"location":"changelog/#310-2019-09-30","title":"3.1.0 / 2019-09-30","text":"<p>Adds a new dynamic programming algorithm to the suite of paths.</p> <p>New Features</p> <ul> <li>#102 Adds new <code>dp</code> path.</li> </ul>"},{"location":"changelog/#300-2019-08-10","title":"3.0.0 / 2019-08-10","text":"<p>This release moves <code>opt_einsum</code> to be backend agnostic while adding support additional backends such as Jax and Autograd. Support for Python 2.7 has been dropped and Python 3.5 will become the new minimum version, a Python deprecation policy equivalent to NumPy's has been adopted.</p> <p>New Features</p> <ul> <li>#78 A new random-optimizer has been implemented which uses Boltzmann weighting to explore alternative near-minimum paths using greedy-like schemes. This provides a fairly large path performance enhancements with a linear path time overhead.</li> <li>#78 A new PathOptimizer class has been implemented to provide a framework for building new optimizers. An example is that now custom cost functions can now be provided in the greedy formalism for building custom optimizers without a large amount of additional code.</li> <li>#81 The <code>backend=\"auto\"</code> keyword has been implemented for <code>contract</code> allowing automatic detection of the correct backend to use based off provided tensors in the contraction.</li> <li>#88 Autograd and Jax support have been implemented.</li> <li>#96 Deprecates Python 2 functionality and devops improvements.</li> </ul> <p>Enhancements</p> <ul> <li>#84 The <code>contract_path</code> function can now accept shape tuples rather than full tensors.</li> <li>#84 The <code>contract_path</code> automated path algorithm decision technology has been refactored to a standalone function.</li> </ul>"},{"location":"changelog/#230-2018-12-01","title":"2.3.0 / 2018-12-01","text":"<p>This release primarily focuses on expanding the suite of available path technologies to provide better optimization characistics for 4-20 tensors while decreasing the time to find paths for 50-200+ tensors. See <code>Path Overview &lt;path_finding.html#performance-comparison&gt;</code>_ for more information.</p> <p>New Features</p> <ul> <li>#60 A new <code>greedy</code> implementation has been added which is up to two orders of magnitude faster for 200 tensors.</li> <li>#73 Adds a new <code>branch</code> path that uses <code>greedy</code> ideas to prune the <code>optimal</code> exploration space to provide a better path than <code>greedy</code> at sub <code>optimal</code> cost.</li> <li>#73 Adds a new <code>auto</code> keyword to the <code>opt_einsum.contract</code> <code>path</code> option. This keyword automatically chooses the best path technology that takes under 1ms to execute.</li> </ul> <p>Enhancements</p> <ul> <li>#61 The <code>opt_einsum.contract</code> <code>path</code> keyword has been changed to <code>optimize</code> to more closely match NumPy. <code>path</code> will be deprecated in the future.</li> <li>#61 The <code>opt_einsum.contract_path</code> now returns a <code>opt_einsum.contract.PathInfo</code> object that can be queried for the scaling, flops, and intermediates of the path. The print representation of this object is identical to before.</li> <li>#61 The default <code>memory_limit</code> is now unlimited by default based on community feedback.</li> <li>#66 The Torch backend will now use <code>tensordot</code> when using a version of Torch which includes this functionality.</li> <li>#68 Indices can now be any hashable object when provided in the <code>\"Interleaved Input\" &lt;input_format.html#interleaved-input&gt;</code>_ syntax.</li> <li>#74 Allows the default <code>transpose</code> operation to be overridden to take advantage of more advanced tensor transpose libraries.</li> <li>#73 The <code>optimal</code> path is now significantly faster.</li> <li>#81 A documentation pass for v3.0.</li> </ul> <p>Bug fixes</p> <ul> <li>#72 Fixes the <code>\"Interleaved Input\" &lt;input_format.html#interleaved-input&gt;</code>_ syntax and adds documentation.</li> </ul>"},{"location":"changelog/#220-2018-07-29","title":"2.2.0 / 2018-07-29","text":"<p>New Features</p> <ul> <li>#48 Intermediates can now be shared between contractions, see here for more details.</li> <li>#53 Intermediate caching is thread safe.</li> </ul> <p>Enhancements</p> <ul> <li>#48 Expressions are now mapped to non-unicode index set so that unicode input is support for all backends.</li> <li>#54 General documentation update.</li> </ul> <p>Bug fixes</p> <ul> <li>#41 PyTorch indices are mapped back to a small a-z subset valid for PyTorch's einsum implementation.</li> </ul>"},{"location":"changelog/#213-2018-8-23","title":"2.1.3 / 2018-8-23","text":"<p>Bug fixes</p> <ul> <li>Fixes unicode issue for large numbers of tensors in Python 2.7.</li> <li>Fixes unicode install bug in README.md.</li> </ul>"},{"location":"changelog/#212-2018-8-16","title":"2.1.2 / 2018-8-16","text":"<p>Bug fixes</p> <ul> <li>Ensures <code>versioneer.py</code> is in MANIFEST.in for a clean pip install.</li> </ul>"},{"location":"changelog/#211-2018-8-15","title":"2.1.1 / 2018-8-15","text":"<p>Bug fixes</p> <ul> <li>Corrected Markdown display on PyPi.</li> </ul>"},{"location":"changelog/#210-2018-8-15","title":"2.1.0 / 2018-8-15","text":"<p><code>opt_einsum</code> continues to improve its support for additional backends beyond NumPy with PyTorch.</p> <p>We have also published the opt_einsum package in the Journal of Open Source Software. If you use this package in your work, please consider citing us!</p> <p>New features</p> <ul> <li>PyTorch backend support</li> <li>Tensorflow eager-mode execution backend support</li> </ul> <p>Enhancements</p> <ul> <li>Intermediate tensordot-like expressions are now ordered to avoid transposes.</li> <li>CI now uses conda backend to better support GPU and tensor libraries.</li> <li>Now accepts arbitrary unicode indices rather than a subset.</li> <li>New auto path option which switches between optimal and greedy at four tensors.</li> </ul> <p>Bug fixes</p> <ul> <li>Fixed issue where broadcast indices were incorrectly locked out of tensordot-like evaluations even after their dimension was broadcast.</li> </ul>"},{"location":"changelog/#201-2018-6-28","title":"2.0.1 / 2018-6-28","text":"<p>New Features</p> <ul> <li>Allows unlimited Unicode indices.</li> <li>Adds a Journal of Open-Source Software paper.</li> <li>Minor documentation improvements.</li> </ul>"},{"location":"changelog/#200-2018-5-17","title":"2.0.0 / 2018-5-17","text":"<p><code>opt_einsum</code> is a powerful tensor contraction order optimizer for NumPy and related ecosystems.</p> <p>New Features</p> <ul> <li>Expressions can be precompiled so that the expression optimization need not happen multiple times.</li> <li>The greedy order optimization algorithm has been tuned to be able to handle hundreds of tensors in several seconds.</li> <li>Input indices can now be unicode so that expressions can have many thousands of indices.</li> <li>GPU and distributed computing backends have been added such as Dask, TensorFlow, CUPy, Theano, and Sparse.</li> </ul> <p>Bug Fixes</p> <ul> <li>An error affecting cases where opt_einsum mistook broadcasting operations for matrix multiply has been fixed.</li> <li>Most error messages are now more expressive.</li> </ul>"},{"location":"changelog/#100-2016-10-14","title":"1.0.0 / 2016-10-14","text":"<p>Einsum is a very powerful function for contracting tensors of arbitrary dimension and index. However, it is only optimized to contract two terms at a time resulting in non-optimal scaling for contractions with many terms. Opt_einsum aims to fix this by optimizing the contraction order which can lead to arbitrarily large speed ups at the cost of additional intermediate tensors.</p> <p>Opt_einsum is also implemented into the np.einsum function as of NumPy v1.12.</p> <p>New Features</p> <ul> <li>Tensor contraction order optimizer.</li> <li><code>opt_einsum.contract</code> as a drop-in replacement for <code>numpy.einsum</code>.</li> </ul>"},{"location":"examples/dask_reusing_intermediaries/","title":"Reusing Intermediaries with Dask","text":"<p>Dask provides a computational framework where arrays and the computations on them are built up into a 'task graph' before computation. Since :mod:<code>opt_einsum</code> is compatible with <code>dask</code> arrays this means that multiple contractions can be built into the same task graph, which then automatically reuses any shared arrays and contractions.</p> <p>For example, imagine the two expressions:</p> <pre><code>contraction1 = 'ab,dca,eb,cde'\ncontraction2 = 'ab,cda,eb,cde'\nsizes = {l: 10 for l in 'abcde'}\n</code></pre> <p>The contraction <code>'ab,eb'</code> is shared between them and could only be done once. First, let's set up some <code>numpy</code> arrays:</p> <pre><code>terms1, terms2 = contraction1.split(','), contraction2.split(',')\nterms = set((*terms1, *terms2))\nterms\n#&gt; {'ab', 'cda', 'cde', 'dca', 'eb'}\n\nimport numpy as np\nnp_arrays = {s: np.random.randn(*(sizes[c] for c in s)) for s in terms}\n# filter the arrays needed for each expression\nnp_ops1 = [np_arrays[s] for s in terms1]\nnp_ops2 = [np_arrays[s] for s in terms2]\n</code></pre> <p>Typically we would compute these expressions separately:</p> <pre><code>oe.contract(contraction1, *np_ops1)\n#&gt; array(114.78314052)\n\noe.contract(contraction2, *np_ops2)\n#&gt; array(-75.55902751)\n</code></pre> <p>However, if we use dask arrays we can combine the two operations, so let's set those up:</p> <pre><code>import dask.array as da\nda_arrays = {s: da.from_array(np_arrays[s], chunks=1000, name=s) for s in inputs}\nda_arrays\n#&gt; {'ab': dask.array&lt;ab, shape=(10, 10), dtype=float64, chunksize=(10, 10)&gt;,\n#&gt;  'cda': dask.array&lt;cda, shape=(10, 10, 10), dtype=float64, chunksize=(10, 10, 10)&gt;,\n#&gt;  'cde': dask.array&lt;cde, shape=(10, 10, 10), dtype=float64, chunksize=(10, 10, 10)&gt;,\n#&gt;  'dca': dask.array&lt;dca, shape=(10, 10, 10), dtype=float64, chunksize=(10, 10, 10)&gt;,\n#&gt;  'eb': dask.array&lt;eb, shape=(10, 10), dtype=float64, chunksize=(10, 10)&gt;}\n\nda_ops1 = [da_arrays[s] for s in terms1]\nda_ops2 = [da_arrays[s] for s in terms2]\n</code></pre> <p>Note <code>chunks</code> is a required argument relating to how the arrays are stored (see array-creation). Now we can perform the contraction:</p> <pre><code># these won't be immediately evaluated\ndy1 = oe.contract(contraction1, *da_ops1, backend='dask')\ndy2 = oe.contract(contraction2, *da_ops2, backend='dask')\n\n# wrap them in delayed to combine them into the same computation\nfrom dask import delayed\ndy = delayed([dy1, dy2])\ndy\n#&gt; Delayed('list-3af82335-b75e-47d6-b800-68490fc865fd')\n</code></pre> <p>As suggested by the name <code>Delayed</code>, we have a placeholder for the result so far. When we want to perform the computation we can call:</p> <pre><code>dy.compute()\n#&gt; [114.78314052155015, -75.55902750513113]\n</code></pre> <p>The above matches the canonical numpy result. The computation can even be handled by various schedulers - see scheduling. Finally, to check we are reusing intermediaries, we can view the task graph generated for the computation:</p> <pre><code>dy.visualize(optimize_graph=True)\n</code></pre> <p></p> <p>Note</p> <p>For sharing intermediates with other backends see Sharing Intermediates. Dask graphs are particularly useful for reusing intermediates beyond just contractions and can allow additional parallelization.</p>"},{"location":"examples/large_expr_with_greedy/","title":"Large Expressions with Greedy","text":"<p>Using the greedy method allows the contraction of hundreds of tensors. Here's an example from quantum of computing the inner product between two 'Matrix Product States'. Graphically, if we represent each tensor as an <code>O</code>, give it the same number of 'legs' as it has indices, and join those legs when that index is summed with another tensor, we get an expression for <code>n</code> particles that looks like:</p> <pre><code>O-O-O-O-O-O-     -O-O-O-O-O-O\n| | | | | |  ...  | | | | | |\nO-O-O-O-O-O-     -O-O-O-O-O-O\n\n0 1 2 3 4 5 ........... n-2 n-1\n</code></pre> <p>The meaning of this is not that important other than its a large, useful contraction. For <code>n=100</code> it involves 200 different tensors and about 300 unique indices. With this many indices it can be useful to generate them with the function <code>opt_einsum.parser.get_symbol</code>.</p>"},{"location":"examples/large_expr_with_greedy/#setup-the-string","title":"Setup the string","text":"<pre><code>import numpy as np\nimport opt_einsum as oe\n\nn = 100\nphys_dim = 3\nbond_dim = 10\n\n# start with first site\n# O--\n# |\n# O--\neinsum_str = \"ab,ac,\"\n\nfor i in range(1, n - 1):\n    # set the upper left/right, middle and lower left/right indices\n    # --O--\n    #   |\n    # --O--\n    j = 3 * i\n    ul, ur, m, ll, lr = (oe.get_symbol(i)\n                         for i in (j - 1, j + 2, j, j - 2, j + 1))\n    einsum_str += \"{}{}{},{}{}{},\".format(m, ul, ur, m, ll, lr)\n\n# finish with last site\n# --O\n#   |\n# --O\ni = n - 1\nj = 3 * i\nul, m, ll, =  (oe.get_symbol(i) for i in (j - 1, j, j - 2))\neinsum_str += \"{}{},{}{}\".format(m, ul, m, ll)\n</code></pre>"},{"location":"examples/large_expr_with_greedy/#generate-the-shapes","title":"Generate the shapes","text":"<pre><code>def gen_shapes():\n    yield (phys_dim, bond_dim)\n    yield (phys_dim, bond_dim)\n    for i in range(1, n - 1):\n        yield(phys_dim, bond_dim, bond_dim)\n        yield(phys_dim, bond_dim, bond_dim)\n    yield (phys_dim, bond_dim)\n    yield (phys_dim, bond_dim)\n\nshapes = tuple(gen_shapes())\n</code></pre> <p>Let's time how long it takes to generate the expression (<code>'greedy'</code> is used by default, and we turn off the <code>memory_limit</code>):</p> <pre><code>%timeit expr = oe.contract_expression(einsum_str, *shapes, memory_limit=-1)\n#&gt; 76.2 ms \u00b1 1.05 ms per loop (mean \u00b1 std. dev. of 7 runs, 10 loops each)\n</code></pre> <p>This is pretty manageable, though we might want to think about splitting the expression up if we go a lot bigger. Importantly, we can then use this repeatedly with any set of matching arrays:</p> <pre><code>arrays = [np.random.randn(*shp) / 4 for shp in shapes]\nexpr(*arrays)\n#&gt; array(23.23628116)\n\narrays = [np.random.randn(*shp) / 4 for shp in shapes]\nexpr(*arrays)\n#&gt; array(-12.21091879)\n</code></pre>"},{"location":"examples/large_expr_with_greedy/#full-path","title":"Full path","text":"<p>And if we really want we can generate the full contraction path info:</p> <pre><code>print(oe.contract_path(einsum_str, *arrays, memory_limit=-1)[1])\n#&gt;   Complete contraction:  ab,ac,dcf,dbe,gfi,geh,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,\u01b3\u01b2\u01b5,\u01b3\u01b1\u01b4,\u01b6\u01b5,\u01b6\u01b4-&gt;\n#&gt;          Naive scaling:  298\n#&gt;      Optimized scaling:  5\n#&gt;       Naive FLOP count:  1.031e+248\n#&gt;   Optimized FLOP count:  1.168e+06\n#&gt;    Theoretical speedup:  88264689284468460017580864156865782413140936705854966013600065426858041248009637246968036807489558012989638169986640870276510490846199301907401763236976204166215471281505344088317454144870323271826022036197984172898402324699098341524952317952.000\n#&gt;   Largest intermediate:  3.000e+02 elements\n#&gt; --------------------------------------------------------------------------------\n#&gt; scaling        BLAS                current                             remaining\n#&gt; --------------------------------------------------------------------------------\n#&gt;    4           TDOT            dbe,ab-&gt;ade ac,dcf,gfi,geh,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,\u01b3\u01b2\u01b5,\u01b3\u01b1\u01b4,\u01b6\u01b5,\u01b6\u01b4,ade-&gt;\n#&gt;    4           TDOT            dcf,ac-&gt;adf gfi,geh,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,\u01b3\u01b2\u01b5,\u01b3\u01b1\u01b4,\u01b6\u01b5,\u01b6\u01b4,ade,adf-&gt;\n#&gt;    4           GEMM            \u01b6\u01b5,\u01b3\u01b2\u01b5-&gt;\u01b3\u01b6\u01b2 gfi,geh,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,\u01b3\u01b1\u01b4,\u01b6\u01b4,ade,adf,\u01b3\u01b6\u01b2-&gt;\n#&gt;    4           GEMM            \u01b6\u01b4,\u01b3\u01b1\u01b4-&gt;\u01b3\u01b6\u01b1 gfi,geh,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,ade,adf,\u01b3\u01b6\u01b2,\u01b3\u01b6\u01b1-&gt;\n#&gt;    5           TDOT          ade,geh-&gt;adgh gfi,jil,jhk,mlo,mkn,por,pnq,sru,sqt,vux,vtw,yxA,ywz,BAD,BzC,EDG,ECF,HGJ,HFI,KJM,KIL,NMP,NLO,QPS,QOR,TSV,TRU,WVY,WUX,ZY\u00c2,ZX\u00c1,\u00c3\u00c2\u00c5,\u00c3\u00c1\u00c4,\u00c6\u00c5\u00c8,\u00c6\u00c4\u00c7,\u00c9\u00c8\u00cb,\u00c9\u00c7\u00ca,\u00cc\u00cb\u00ce,\u00cc\u00ca\u00cd,\u00cf\u00ce\u00d1,\u00cf\u00cd\u00d0,\u00d2\u00d1\u00d4,\u00d2\u00d0\u00d3,\u00d5\u00d4\u00d7,\u00d5\u00d3\u00d6,\u00d8\u00d7\u00da,\u00d8\u00d6\u00d9,\u00db\u00da\u00dd,\u00db\u00d9\u00dc,\u00de\u00dd\u00e0,\u00de\u00dc\u00df,\u00e1\u00e0\u00e3,\u00e1\u00df\u00e2,\u00e4\u00e3\u00e6,\u00e4\u00e2\u00e5,\u00e7\u00e6\u00e9,\u00e7\u00e5\u00e8,\u00ea\u00e9\u00ec,\u00ea\u00e8\u00eb,\u00ed\u00ec\u00ef,\u00ed\u00eb\u00ee,\u00f0\u00ef\u00f2,\u00f0\u00ee\u00f1,\u00f3\u00f2\u00f5,\u00f3\u00f1\u00f4,\u00f6\u00f5\u00f8,\u00f6\u00f4\u00f7,\u00f9\u00f8\u00fb,\u00f9\u00f7\u00fa,\u00fc\u00fb\u00fe,\u00fc\u00fa\u00fd,\u00ff\u00fe\u0101,\u00ff\u00fd\u0100,\u0102\u0101\u0104,\u0102\u0100\u0103,\u0105\u0104\u0107,\u0105\u0103\u0106,\u0108\u0107\u010a,\u0108\u0106\u0109,\u010b\u010a\u010d,\u010b\u0109\u010c,\u010e\u010d\u0110,\u010e\u010c\u010f,\u0111\u0110\u0113,\u0111\u010f\u0112,\u0114\u0113\u0116,\u0114\u0112\u0115,\u0117\u0116\u0119,\u0117\u0115\u0118,\u011a\u0119\u011c,\u011a\u0118\u011b,\u011d\u011c\u011f,\u011d\u011b\u011e,\u0120\u011f\u0122,\u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0126\u0125\u0128,\u0126\u0124\u0127,\u0129\u0128\u012b,\u0129\u0127\u012a,\u012c\u012b\u012e,\u012c\u012a\u012d,\u012f\u012e\u0131,\u012f\u012d\u0130,\u0132\u0131\u0134,\u0132\u0130\u0133,\u0135\u0134\u0137,\u0135\u0133\u0136,\u0138\u0137\u013a,\u0138\u0136\u0139,\u013b\u013a\u013d,\u013b\u0139\u013c,\u013e\u013d\u0140,\u013e\u013c\u013f,\u0141\u0140\u0143,\u0141\u013f\u0142,\u0144\u0143\u0146,\u0144\u0142\u0145,\u0147\u0146\u0149,\u0147\u0145\u0148,\u014a\u0149\u014c,\u014a\u0148\u014b,\u014d\u014c\u014f,\u014d\u014b\u014e,\u0150\u014f\u0152,\u0150\u014e\u0151,\u0153\u0152\u0155,\u0153\u0151\u0154,\u0156\u0155\u0158,\u0156\u0154\u0157,\u0159\u0158\u015b,\u0159\u0157\u015a,\u015c\u015b\u015e,\u015c\u015a\u015d,\u015f\u015e\u0161,\u015f\u015d\u0160,\u0162\u0161\u0164,\u0162\u0160\u0163,\u0165\u0164\u0167,\u0165\u0163\u0166,\u0168\u0167\u016a,\u0168\u0166\u0169,\u016b\u016a\u016d,\u016b\u0169\u016c,\u016e\u016d\u0170,\u016e\u016c\u016f,\u0171\u0170\u0173,\u0171\u016f\u0172,\u0174\u0173\u0176,\u0174\u0172\u0175,\u0177\u0176\u0179,\u0177\u0175\u0178,\u017a\u0179\u017c,\u017a\u0178\u017b,\u017d\u017c\u017f,\u017d\u017b\u017e,\u0180\u017f\u0182,\u0180\u017e\u0181,\u0183\u0182\u0185,\u0183\u0181\u0184,\u0186\u0185\u0188,\u0186\u0184\u0187,\u0189\u0188\u018b,\u0189\u0187\u018a,\u018c\u018b\u018e,\u018c\u018a\u018d,\u018f\u018e\u0191,\u018f\u018d\u0190,\u0192\u0191\u0194,\u0192\u0190\u0193,\u0195\u0194\u0197,\u0195\u0193\u0196,\u0198\u0197\u019a,\u0198\u0196\u0199,\u019b\u019a\u019d,\u019b\u0199\u019c,\u019e\u019d\u01a0,\u019e\u019c\u019f,\u01a1\u01a0\u01a3,\u01a1\u019f\u01a2,\u01a4\u01a3\u01a6,\u01a4\u01a2\u01a5,\u01a7\u01a6\u01a9,\u01a7\u01a5\u01a8,\u01aa\u01a9\u01ac,\u01aa\u01a8\u01ab,\u01ad\u01ac\u01af,\u01ad\u01ab\u01ae,\u01b0\u01af\u01b2,\u01b0\u01ae\u01b1,adf,\u01b3\u01b6\u01b2,\u01b3\u01b6\u01b1,adgh-&gt;\n#&gt; \n#&gt;    ...\n#&gt; \n#&gt;    4           TDOT            \u011e\u011f,\u0120\u011f\u0122-&gt;\u0120\u011e\u0122                  \u0120\u011e\u0121,\u0123\u0122\u0125,\u0123\u0121\u0124,\u0124\u0125,\u0120\u011e\u0122-&gt;\n#&gt;    4           GEMM            \u0120\u011e\u0122,\u0120\u011e\u0121-&gt;\u0121\u0122                       \u0123\u0122\u0125,\u0123\u0121\u0124,\u0124\u0125,\u0121\u0122-&gt;\n#&gt;    4           GEMM            \u0124\u0125,\u0123\u0122\u0125-&gt;\u0123\u0122\u0124                          \u0123\u0121\u0124,\u0121\u0122,\u0123\u0122\u0124-&gt;\n#&gt;    4           TDOT            \u0123\u0122\u0124,\u0123\u0121\u0124-&gt;\u0121\u0122                               \u0121\u0122,\u0121\u0122-&gt;\n#&gt;    2            DOT                \u0121\u0122,\u0121\u0122-&gt;                                    -&gt;\n</code></pre> <p>Where we can see the speedup over a naive einsum is about <code>10^241</code>, not bad!</p>"},{"location":"getting_started/backends/","title":"Backends &amp; GPU Support","text":"<p><code>opt_einsum</code> is largely agnostic to the type of n-dimensional arrays (tensors) it uses, since finding the contraction path only relies on getting the shape attribute of each array supplied. It can perform the underlying tensor contractions with various libraries. In fact, any library that provides a <code>numpy.tensordot</code> and <code>numpy.transpose</code> implementation can perform most normal contractions. However, certain special functionalities such as axes reduction are reliant on a <code>numpy.einsum</code> implementation. The following is a brief overview of libraries which have been tested with <code>opt_einsum</code>:</p> <ul> <li>tensorflow: compiled tensor expressions   that can run on GPU.</li> <li>theano: compiled tensor   expressions that can run on GPU.</li> <li>cupy: numpy-like api for GPU tensors.</li> <li>dask: larger-than-memory tensor   computations, distributed scheduling, and potential reuse of   intermediaries.</li> <li>sparse: sparse tensors.</li> <li>pytorch: numpy-like api for GPU tensors.</li> <li>autograd: automatic derivative   computation for tensor expressions</li> <li>jax: compiled GPU tensor expressions   including <code>autograd</code>-like functionality</li> </ul> <p>Note</p> <p>For a contraction to be possible without using a backend einsum, it must satisfy the following rule: in the full expression (including output indices) each index must appear twice. In other words, each dimension must be either contracted with one other dimension or left alone.</p>"},{"location":"getting_started/backends/#backend-agnostic-contractions","title":"Backend agnostic contractions","text":"<p>The automatic backend detection will be detected based on the first supplied array (default), this can be overridden by specifying the correct <code>backend</code> argument for the type of arrays supplied when calling <code>opt_einsum.contract</code>. For example, if you had a library installed called <code>'foo'</code> which provided an <code>numpy.ndarray</code> like object with a <code>.shape</code> attribute as well as <code>foo.tensordot</code> and <code>foo.transpose</code> then you could contract them with something like:</p> <pre><code>contract(einsum_str, *foo_arrays, backend='foo')\n</code></pre> <p>Behind the scenes <code>opt_einsum</code> will find the contraction path, perform pairwise contractions using e.g. <code>foo.tensordot</code> and finally return the canonical type those functions return.</p>"},{"location":"getting_started/backends/#dask","title":"Dask","text":"<p>dask is an example of a library which satisfies these requirements. For example:</p> <pre><code>import opt_einsum as oe\nimport dask.array as da\nshapes = (3, 200), (200, 300), (300, 4)\ndxs = [da.random.normal(0, 1, shp, chunks=(100, 100)) for shp in shapes]\ndxs\n#&gt; [dask.array&lt;da.random.normal, shape=(3, 200), dtype=float64, chunksize=(3, 100)&gt;,\n#&gt;  dask.array&lt;da.random.normal, shape=(200, 300), dtype=float64, chunksize=(100, 100)&gt;,\n#&gt;  dask.array&lt;da.random.normal, shape=(300, 4), dtype=float64, chunksize=(100, 4)&gt;]\n\n\ndy = oe.contract(\"ab,bc,cd\", *dxs)  # will infer backend='dask'\ndy\n#&gt; dask.array&lt;transpose, shape=(3, 4), dtype=float64, chunksize=(3, 4)&gt;\n\ndy.compute()\n#&gt; array([[ 470.71404665,    2.44931372,  -28.47577265,  424.37716615],\n#&gt;        [  64.38328345, -287.40753131,  144.46515642,  324.88169821],\n#&gt;        [-142.07153553, -180.41739259,  125.0973783 , -239.16754541]])\n</code></pre> <p>In this case, dask arrays in = dask array out, since dask arrays have a shape attribute, and <code>opt_einsum</code> can find <code>dask.array.tensordot</code> and <code>dask.array.transpose</code>.</p>"},{"location":"getting_started/backends/#sparse","title":"Sparse","text":"<p>The sparse library also fits the requirements and is supported. An example:</p> <pre><code>import sparse as sp\nshapes = (3, 200), (200, 300), (300, 4)\nsxs = [sp.random(shp) for shp in shapes]\nsxs\n#&gt; [&lt;COO: shape=(3, 200), dtype=float64, nnz=6, sorted=False, duplicates=True&gt;,\n#&gt;  &lt;COO: shape=(200, 300), dtype=float64, nnz=600, sorted=False, duplicates=True&gt;,\n#&gt;  &lt;COO: shape=(300, 4), dtype=float64, nnz=12, sorted=False, duplicates=True&gt;]\n\noe.contract(\"ab,bc,cd\", *sxs)\n#&gt; &lt;COO: shape=(3, 4), dtype=float64, nnz=0, sorted=False, duplicates=False&gt;\n</code></pre>"},{"location":"getting_started/backends/#autograd","title":"Autograd","text":"<p>The autograd library is a drop-in for <code>numpy</code> that can automatically compute the gradients of array expressions. <code>opt_einsum</code> automatically dispatches the <code>autograd</code> arrays correctly, enabling a simple way to compute gradients of tensor contractions:</p> <pre><code>import numpy as np\nimport autograd\nshapes = [(2, 3), (3, 4), (4, 2)]\nx, y, z = [np.random.rand(*s) for s in shapes]\n\n# make single arg function as autograd takes derivative of first arg\ndef foo(xyz):\n   return oe.contract('ij,jk,ki-&gt;', *xyz)\n\nfoo([x, y, z])\n#&gt; array(4.90422159)\n\n# wrap foo with autograd to compute gradients instead\ndfoo = autograd.grad(foo)\ndx, dy, dz = dfoo(arrays)\ndx, dy, dz\n#&gt; (array([[1.10056194, 1.25078356, 1.48211494],\n#&gt;         [1.38945961, 1.5572077 , 1.65234003]]),\n#&gt;  array([[0.41710717, 0.63202881, 0.84573502, 0.95069975],\n#&gt;         [0.42706777, 0.73630994, 0.99328938, 0.77415267],\n#&gt;         [0.40773334, 0.61693475, 0.82545726, 0.93132302]]),\n#&gt;  array([[0.78747828, 1.28979012],\n#&gt;         [1.26051133, 1.48835538],\n#&gt;         [0.46896666, 0.55003072],\n#&gt;         [1.10840828, 1.16722494]]))\n</code></pre>"},{"location":"getting_started/backends/#jax","title":"Jax","text":"<p>jax is itself a drop-in for <code>autograd</code>, that additionally uses XLA to compile the expressions, particularly for the GPU. Using it with <code>opt_einsum</code> is very simple:</p> <pre><code>import jax\n# generate a compiled version of the above function\njit_foo = jax.jit(foo)\njit_foo([x, y, z])\n#&gt; DeviceArray(4.9042215, dtype=float32)\n\n# generate a compiled version of the gradient function\njit_dfoo = jax.jit(jax.grad(foo))\njit_dfoo([x, y, z])\n#&gt; [DeviceArray([[1.10056198, 1.25078356, 1.48211491],\n#&gt;               [1.38945973, 1.5572077, 1.65234005]], dtype=float32),\n#&gt;  DeviceArray([[0.41710716, 0.63202882, 0.84573501, 0.95069975],\n#&gt;               [0.42706776, 0.73630995, 0.99328935, 0.7741527 ],\n#&gt;               [0.40773335, 0.61693472, 0.82545722, 0.93132305]],\n#&gt;              dtype=float32),\n#&gt;  DeviceArray([[0.78747827, 1.28979015],\n#&gt;               [1.2605114 , 1.4883554 ],\n#&gt;               [0.46896666, 0.55003077],\n#&gt;               [1.10840821, 1.16722488]], dtype=float32)]\n</code></pre> <p>Note</p> <p><code>jax</code> defaults to converting all arrays to single precision. This behaviour can be changed by running <code>from jax.config import config; config.update(\"jax_enable_x64\", True)</code> before it has been imported and used at all.</p>"},{"location":"getting_started/backends/#special-gpu-backends-for-numpy-arrays","title":"Special (GPU) backends for numpy arrays","text":"<p>A particular case is if numpy arrays are required for the input and output, however, a more performant backend is required such as performing the contraction on a GPU. Unless the specified backend works on numpy arrays, this requires converting to and from the backend array type. Currently <code>opt_einsum</code> can handle this automatically for:</p> <ul> <li>tensorflow</li> <li>theano</li> <li>cupy</li> <li>pytorch</li> <li>jax</li> </ul> <p>all of which offer GPU support. Since <code>tensorflow</code> and <code>theano</code> both require compiling the expression, this functionality is encapsulated in generating a <code>opt_einsum.ContractExpression</code> using <code>opt_einsum.contract_expression</code>, which can then be called using numpy arrays whilst specifying <code>backend='tensorflow'</code> etc. Additionally, if arrays are marked as <code>constant</code> (see <code>constants-section</code>), then these arrays will be kept on the device for optimal performance.</p>"},{"location":"getting_started/backends/#theano","title":"Theano","text":"<p>If <code>theano</code> is installed, using it as backend is as simple as specifying <code>backend='theano'</code>:</p> <pre><code>shapes = (3, 200), (200, 300), (300, 4)\nexpr = oe.contract_expression(\"ab,bc,cd\", *shapes)\nexpr\n#&gt; &lt;ContractExpression('ab,bc,cd')&gt;\n\nimport numpy as np\n# GPU advantage mainly for low precision numbers\nxs = [np.random.randn(*shp).astype(np.float32) for shp in shapes]\nexpr(*xs, backend='theano')  # might see some fluff on first run\n#&gt; array([[ 129.28352  , -128.00702  , -164.62917  , -335.11682  ],\n#&gt;        [-462.52344  , -121.12657  ,  -67.847626 ,  624.5457   ],\n#&gt;        [   5.2838974,   36.441578 ,   81.62851  ,  703.1576   ]],\n#&gt;       dtype=float32)\n</code></pre> <p>Note that you can still supply <code>theano.tensor.TensorType</code> directly to <code>opt_einsum</code> (with <code>backend='theano'</code>), and it will return the relevant <code>theano</code> type.</p>"},{"location":"getting_started/backends/#tensorflow","title":"Tensorflow","text":"<p>To run the expression with tensorflow, you need to register a default session:</p> <pre><code>import tensorflow as tf\nsess = tf.Session()\n\nwith sess.as_default():\n    out = expr(*xs, backend='tensorflow')\n\nout\n#&gt; array([[ 129.28357  , -128.00684  , -164.62903  , -335.1167   ],\n#&gt;        [-462.52362  , -121.12659  ,  -67.84769  ,  624.5455   ],\n#&gt;        [   5.2839584,   36.44155  ,   81.62852  ,  703.15784  ]],\n#&gt;       dtype=float32)\n</code></pre> <p>Note that you can still supply this expression with, for example, a <code>tensorflow.placeholder</code> using <code>backend='tensorflow'</code>, and then no conversion would take place, instead you'd get a <code>tensorflow.Tensor</code> back.</p> <p>Version 1.9 of tensorflow also added support for eager execution of computations. If compilation of the contraction expression tensorflow graph is taking a substantial amount of time up then it can be advantageous to use this, especially since tensor contractions are quite compute-bound. This is achieved by running the following snippet:</p> <pre><code>import tensorflow as tf\ntf.enable_eager_execution()\n</code></pre> <p>After which <code>opt_einsum</code> will automatically detect eager mode if <code>backend='tensorflow'</code> is supplied to a <code>opt_einsum.ContractExpression</code>.</p>"},{"location":"getting_started/backends/#pytorch-cupy","title":"Pytorch &amp; Cupy","text":"<p>Both pytorch and cupy offer numpy-like, GPU-enabled arrays which execute eagerly rather than requiring any compilation. If they are installed, no steps are required to utilize them other than specifying the <code>backend</code> keyword:</p> <pre><code>expr(*xs, backend='torch')\n#&gt; array([[ 129.28357  , -128.00684  , -164.62903  , -335.1167   ],\n#&gt;        [-462.52362  , -121.12659  ,  -67.84769  ,  624.5455   ],\n#&gt;        [   5.2839584,   36.44155  ,   81.62852  ,  703.15784  ]],\n#&gt;       dtype=float32)\n\nexpr(*xs, backend='cupy')\n#&gt; array([[ 129.28357  , -128.00684  , -164.62903  , -335.1167   ],\n#&gt;        [-462.52362  , -121.12659  ,  -67.84769  ,  624.5455   ],\n#&gt;        [   5.2839584,   36.44155  ,   81.62852  ,  703.15784  ]],\n#&gt;       dtype=float32)\n</code></pre> <p>And as with the other GPU backends, if raw <code>cupy</code> or <code>pytorch</code> arrays are supplied the returned array will be of the same type, with no conversion to or from <code>numpy</code> arrays.</p>"},{"location":"getting_started/backends/#jax_1","title":"Jax","text":"<p>jax, as introduced above, can compile tensor functions, in doing so often achieving better performance. <code>opt_einsum</code> expressions can handle this behind the scenes, so again just the <code>backend</code> keyword needs to be supplied:</p> <pre><code>expr(*xs, backend='jax')\n#&gt; array([[ 129.28357  , -128.00684  , -164.62903  , -335.1167   ],\n#&gt;        [-462.52362  , -121.12659  ,  -67.84769  ,  624.5455   ],\n#&gt;        [   5.2839584,   36.44155  ,   81.62852  ,  703.15784  ]],\n#&gt;       dtype=float32)\n</code></pre>"},{"location":"getting_started/backends/#contracting-arbitrary-objects","title":"Contracting arbitrary objects","text":"<p>There is one more explicit backend that can handle arbitrary arrays of objects, so long the objects themselves just support multiplication and addition ( <code>__mul__</code> and <code>__add__</code> dunder methods respectively). Use it by supplying <code>backend='object'</code>.</p> <p>For example, imagine we want to perform a contraction of arrays made up of sympy symbols:</p> <pre><code>import opt_einsum as oe\nimport numpy as np\nimport sympy\n\n# define the symbols\na, b, c, d, e, f, g, h, i, j, k, l = [sympy.symbols(oe.get_symbol(i)) for i in range(12)]\na * b + c * d\n\ud835\udc51\n\n# define the tensors (you might explicitly specify `dtype=object`)\nX = np.array([[a, b], [c, d]])\nY = np.array([[e, f], [g, h]])\nZ = np.array([[i, j], [k, l]])\n\n# contract the tensors!\noe.contract('uv,vw,wu-&gt;u', X, Y, Z, backend='object')\n# array([i*(a*e + b*g) + k*(a*f + b*h), j*(c*e + d*g) + l*(c*f + d*h)],\n#       dtype=object)\n</code></pre> <p>There are a few things to note here:</p> <ul> <li>The returned array is a <code>numpy.ndarray</code> but since it has <code>dtype=object</code>   it can really hold any python objects</li> <li>We had to explicitly use <code>backend='object'</code>, since <code>numpy.einsum</code>   would have otherwise been dispatched to, which can't handle <code>dtype=object</code>   (though <code>numpy.tensordot</code> in fact can)</li> <li>Although an optimized pairwise contraction order is used, the looping in each   single contraction is performed in python so performance will be   drastically lower than for numeric dtypes!</li> </ul>"},{"location":"getting_started/input_format/","title":"Input Format","text":"<p>The <code>opt_einsum</code> package was originally designed as a drop-in replacement for the <code>np.einsum</code> function and supports all input formats that <code>np.einsum</code> supports. There are two styles of input accepted, a basic introduction to which can be found in the documentation for <code>numpy.einsum</code>. In addition to this, <code>opt_einsum</code> extends the allowed index labels to unicode or arbitrary hashable, comparable objects in order to handle large contractions with many indices.</p>"},{"location":"getting_started/input_format/#equation-input","title":"'Equation' Input","text":"<p>As with <code>numpy.einsum</code>, here you specify an equation as a string, followed by the array arguments:</p> <pre><code>import opt_einsum as oe\neq = 'ijk,jkl-&gt;li'\nx, y = np.random.rand(2, 3, 4), np.random.rand(3, 4, 5)\nz = oe.contract(eq, x, y)\nz.shape\n#&gt; (5, 2)\n</code></pre> <p>However, in addition to the standard alphabet, <code>opt_einsum</code> also supports unicode characters:</p> <pre><code>eq = \"\u03b1\u03b2\u03b3,\u03b2\u03b3\u03b4-&gt;\u03b4\u03b1\"\noe.contract(eq, x, y).shape\n#&gt; (5, 2)\n</code></pre> <p>This enables access to thousands of possible index labels. One way to access these programmatically is through the function <code>get_symbols</code>:</p> <pre><code>oe.get_symbol(805)\n#&gt; '\u03b1'\n</code></pre> <p>which maps an <code>int</code> to a unicode characater. Note that as with <code>numpy.einsum</code> if the output is not specified with <code>-&gt;</code> it will default to the sorted order of all indices appearing once:</p> <pre><code>eq = \"\u03b1\u03b2\u03b3,\u03b2\u03b3\u03b4\"  # \"-&gt;\u03b1\u03b4\" is implicit\noe.contract(eq, x, y).shape\n#&gt; (2, 5)\n</code></pre>"},{"location":"getting_started/input_format/#interleaved-input","title":"'Interleaved' Input","text":"<p>The other input format is to 'interleave' the array arguments with their index labels ('subscripts') in pairs, optionally specifying the output indices as a final argument. As with <code>numpy.einsum</code>, integers are allowed as these index labels:</p> <pre><code>oe.contract(x, [1, 2, 3], y, [2, 3, 4], [4, 1]).shape\n#&gt; (5, 2)\n</code></pre> <p>with the default output order again specified by the sorted order of indices appearing once. However, unlike <code>numpy.einsum</code>, in <code>opt_einsum</code> you can also put anything hashable and comparable such as <code>str</code> in the subscript list. A simple example of this syntax is:</p> <pre><code>x, y, z = np.ones((1, 2)), np.ones((2, 2)), np.ones((2, 1))\noe.contract(x, ('left', 'bond1'), y, ('bond1', 'bond2'), z, ('bond2', 'right'), ('left', 'right'))\n#&gt; array([[4.]])\n</code></pre> <p>The subscripts need to be hashable so that <code>opt_einsum</code> can efficiently process them, and they should also be comparable so as to allow a default sorted output. For example:</p> <pre><code>x = np.array([[0, 1], [2, 0]])\n\n# original matrix\noe.contract(x, (0, 1))\n#&gt; array([[0, 1],\n#&gt;        [2, 0]])\n\n# the transpose\noe.contract(x, (1, 0))\n#&gt; array([[0, 2],\n#&gt;        [1, 0]])\n\n# original matrix, consistent behavior\noe.contract(x, ('a', 'b'))\n#&gt; array([[0, 1],\n#&gt;        [2, 0]])\n\n# the transpose, consistent behavior\n&gt;&gt;&gt; oe.contract(x, ('b', 'a'))\n#&gt; array([[0, 2],\n#&gt;        [1, 0]])\n\n# relative sequence undefined, can't determine output\n&gt;&gt;&gt; oe.contract(x, (0, 'a'))\n#&gt; TypeError: For this input type lists must contain either Ellipsis\n#&gt; or hashable and comparable object (e.g. int, str)\n</code></pre>"},{"location":"getting_started/install/","title":"Install opt_einsum","text":"<p>You can install <code>opt_einsum</code> with <code>conda</code>, with <code>pip</code>, or by installing from source.</p>"},{"location":"getting_started/install/#conda","title":"Conda","text":"<p>You can update <code>opt_einsum</code> using <code>conda</code>:</p> <pre><code>conda install opt_einsum -c conda-forge\n</code></pre> <p>This installs <code>opt_einsum</code> and the NumPy dependency.</p> <p>The <code>opt_einsum</code> package is maintained on the conda-forge channel.</p>"},{"location":"getting_started/install/#pip","title":"Pip","text":"<p>To install <code>opt_einsum</code> with <code>pip</code> there are a few options, depending on which dependencies you would like to keep up to date:</p> <ul> <li><code>pip install opt_einsum</code></li> </ul>"},{"location":"getting_started/install/#install-from-source","title":"Install from Source","text":"<p>To install opt_einsum from source, clone the repository from github:</p> <pre><code>git clone https://github.com/dgasmith/opt_einsum.git\ncd opt_einsum\npython setup.py install\n</code></pre> <p>or use <code>pip</code> locally if you want to install all dependencies as well::</p> <pre><code>pip install -e .\n</code></pre>"},{"location":"getting_started/install/#test","title":"Test","text":"<p>Test <code>opt_einsum</code> with <code>py.test</code>:</p> <pre><code>cd opt_einsum\npytest\n</code></pre>"},{"location":"getting_started/reusing_paths/","title":"Reusing Paths","text":"<p>If you expect to use a particular contraction repeatedly, it can make things simpler and more efficient not to compute the path each time. Instead, supplying <code>opt_einsum.contract_expression</code> with the contraction string and the shapes of the tensors generates a <code>opt_einsum.ContractExpression</code> which can then be repeatedly called with any matching set of arrays. For example:</p> <pre><code>my_expr = oe.contract_expression(\"abc,cd,dbe-&gt;ea\", (2, 3, 4), (4, 5), (5, 3, 6))\nprint(my_expr)\n#&gt; &lt;ContractExpression('abc,cd,dbe-&gt;ea')&gt;\n#&gt;   1.  'dbe,cd-&gt;bce' [GEMM]\n#&gt;   2.  'bce,abc-&gt;ea' [GEMM]\n</code></pre> <p>The <code>ContractExpression</code> can be called with 3 arrays that match the original shapes without having to recompute the path:</p> <pre><code>x, y, z = (np.random.rand(*s) for s in [(2, 3, 4), (4, 5), (5, 3, 6)])\nmy_expr(x, y, z)\n#&gt; array([[ 3.08331541,  4.13708916],\n#&gt;        [ 2.92793729,  4.57945185],\n#&gt;        [ 3.55679457,  5.56304115],\n#&gt;        [ 2.6208398 ,  4.39024187],\n#&gt;        [ 3.66736543,  5.41450334],\n#&gt;        [ 3.67772272,  5.46727192]])\n</code></pre> <p>Note that few checks are performed when calling the expression, and while it will work for a set of arrays with the same ranks as the original shapes but differing sizes, it might no longer be optimal.</p>"},{"location":"getting_started/reusing_paths/#specifying-constants","title":"Specifying Constants","text":"<p>Often one generates contraction expressions where some of the tensor arguments will remain constant across many calls. <code>opt_einsum.contract_expression</code> allows you to specify the indices of these constant arguments, allowing <code>opt_einsum</code> to build and then reuse as many constant contractions as possible.</p> <p>Take for example the equation:</p> <pre><code>eq = \"ij,jk,kl,lm,mn-&gt;ni\"\n</code></pre> <p>where we know that only the first and last tensors will vary between calls. We can specify this by marking the middle three as constant - we then need to supply the actual arrays rather than just the shapes to <code>opt_einsum.contract_expression</code>:</p> <pre><code>#           A       B       C       D       E\nshapes = [(9, 5), (5, 5), (5, 5), (5, 5), (5, 8)]\n\n# mark the middle three arrays as constant\nconstants = [1, 2, 3]\n\n# generate the constant arrays\nB, C, D = [np.random.randn(*shapes[i]) for i in constants]\n\n# supplied ops are now mix of shapes and arrays\nops = (9, 5), B, C, D, (5, 8)\n\nexpr = oe.contract_expression(eq, *ops, constants=constants)\nexpr\n#&gt; &lt;ContractExpression('ij,[jk,kl,lm],mn-&gt;ni', constants=[1, 2, 3])&gt;\n</code></pre> <p>The expression now only takes the remaining two arrays as arguments (the tensors with <code>'ij'</code> and <code>'mn'</code> indices), and will store as many reusable constant contractions as possible.</p> <p>.. code:: python</p> <pre><code>A1, E1 = np.random.rand(*shapes[0]), np.random.rand(*shapes[-1])\nout1 = expr(A1, E1)\nout1.shap\n#&gt; (8, 9)\n\nA2, E2 = np.random.rand(*shapes[0]), np.random.rand(*shapes[-1])\nout2 = expr(A2, E2)\nout2.shape\n#&gt; (8, 9)\n\nnp.allclose(out1, out2)\n#&gt; False\n\nprint(expr)\n#&gt; &lt;ContractExpression('ij,[jk,kl,lm],mn-&gt;ni', constants=[1, 2, 3])&gt;\n#&gt;   1.  'jm,mn-&gt;jn' [GEMM]\n#&gt;   2.  'jn,ij-&gt;ni' [GEMM]\n</code></pre> <p>Where we can see that the expression now only has to perform two contractions to compute the output.</p> <p>Note</p> <p>The constant part of an expression is lazily generated upon the first call (specific to each backend), though it can also be explicitly built by calling <code>opt_einsum.contract.ContractExpression.evaluate_constants</code>.</p> <p>We can confirm the advantage of using expressions and constants by timing the following scenarios, first setting <code>A = np.random.rand(*shapes[0])</code> and <code>E = np.random.rand(*shapes[-1])</code>.</p>"},{"location":"getting_started/reusing_paths/#contract-from-scratch","title":"Contract from scratch","text":"<pre><code>%timeit oe.contract(eq, A, B, C, D, E)\n#&gt; 239 \u00b5s \u00b1 5.06 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n</code></pre>"},{"location":"getting_started/reusing_paths/#contraction-with-an-expression-but-no-constants","title":"Contraction with an expression but no constants","text":"<pre><code>expr_no_consts = oe.contract_expression(eq, *shapes)\n%timeit expr_no_consts(A, B, C, D, E)\n#&gt; 76.7 \u00b5s \u00b1 2.47 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</code></pre>"},{"location":"getting_started/reusing_paths/#contraction-with-an-expression-and-constants-marked","title":"Contraction with an expression and constants marked","text":"<pre><code>%timeit expr(A, E)\n#&gt; 40.8 \u00b5s \u00b1 1.22 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n</code></pre> <p>Although this gives us a rough idea, of course the efficiency savings are hugely dependent on the size of the contraction and number of possible constant contractions.</p> <p>We also note that even if there are no constant contractions to perform, it can be very advantageous to specify constant tensors for particular backends. For instance, if a GPU backend is used, the constant tensors will be kept on the device rather than being transferred each time.</p>"},{"location":"getting_started/sharing_intermediates/","title":"Sharing Intermediates","text":"<p>If you want to compute multiple similar contractions with common terms, you can embed them in a <code>opt_einsum.shared_intermediates</code> context. Computations of subexpressions in this context will be memoized, and will be garbage collected when the contexts exits.</p> <p>For example, suppose we want to compute marginals at each point in a factor chain:</p> <pre><code>inputs = 'ab,bc,cd,de,ef'\nfactors = [np.random.rand(1000, 1000) for _ in range(5)]\n\n%%timeit\nmarginals = {output: contract('{}-&gt;{}'.format(inputs, output), *factors)\n             for output in 'abcdef'}\n#&gt; 1 loop, best of 3: 5.82 s per loop\n</code></pre> <p>To share this computation, we can perform all contractions in a shared context:</p> <pre><code>%%timeit\nwith shared_intermediates():\n    marginals = {output: contract('{}-&gt;{}'.format(inputs, output), *factors)\n                 for output in 'abcdef'}\n#&gt; 1 loop, best of 3: 1.55 s per loop\n</code></pre> <p>If it is difficult to fit your code into a context, you can instead save the sharing cache for later reuse.</p> <pre><code>with shared_intermediates() as cache:  # create a cache\n    pass\nmarginals = {}\nfor output in 'abcdef':\n    with shared_intermediates(cache):  # reuse a common cache\n        marginals[output] = contract('{}-&gt;{}'.format(inputs, output), *factors)\ndel cache  # garbage collect intermediates\n</code></pre> <p>Note that sharing contexts can be nested, so it is safe to to use <code>opt_einsum.shared_intermediates</code> in library code without leaking intermediates into user caches.</p> <p>Note</p> <p>By default a cache is thread safe, to share intermediates between threads explicitly pass the same cache to each thread.</p>"},{"location":"paths/branching_path/","title":"The Branching Path","text":"<p>While the <code>optimal</code> path is guaranteed to find the smallest estimate FLOP cost, it spends a lot of time exploring paths which are not likely to result in an optimal path. For instance, outer products are usually not advantageous unless absolutely necessary. Additionally, by trying a 'good' path first, it should be possible to quickly establish a threshold FLOP cost which can then be used to prune many bad paths.</p> <p>The branching strategy (provided by <code>opt_einsum.paths.branch</code>) does this by taking the recursive, depth-first approach of <code>opt_einsum.paths.optimal</code>, whilst also sorting potential contractions based on a heuristic cost, as in <code>opt_einsum.paths.greedy</code>.</p> <p>There are two main flavours:</p> <ul> <li><code>optimize='branch-all'</code>: explore all inner products, starting with   those that look best according to the cost heuristic.</li> <li><code>optimize='branch-2'</code>: similar, but at each step only explore the   estimated best two possible contractions, leading to a maximum of   2^N paths assessed.</li> </ul> <p>In both cases, <code>opt_einsum.paths.branch</code> takes an active approach to pruning paths well before they hit the best total FLOP count, by comparing them to the FLOP count (times some factor) achieved by the best path at the same point in the contraction.</p> <p>There is also <code>'branch-1'</code>, which, since it only explores a single path at each step does not really 'branch' - this is essentially the approach of <code>'greedy'</code>. In comparison, <code>'branch-1'</code> will be slower for large expressions, but for small to medium expressions it might find slightly higher quality contractions due to considering individual flop costs at each step.</p> <p>The default <code>optimize='auto'</code> mode of <code>opt_einsum</code> will use <code>'branch-all'</code> for 5 or 6 tensors, though it should be able to handle 12-13 tensors in a matter or seconds. Likewise, <code>'branch-2'</code> will be used for 7 or 8 tensors, though it should be able to handle 20-22 tensors in a matter of seconds. Finally, <code>'branch-1'</code> will be used by <code>'auto'</code> for expressions of up to 14 tensors.</p>"},{"location":"paths/branching_path/#customizing-the-branching-path","title":"Customizing the Branching Path","text":"<p>The 'branch and bound' path can be customized by creating a custom <code>opt_einsum.paths.BranchBound</code> instance. For example:</p> <pre><code>optimizer = oe.BranchBound(nbranch=3, minimize='size', cutoff_flops_factor=None)\npath, path_info = oe.contract_path(eq, *arrays, optimize=optimizer)\n</code></pre> <p>You could then tweak the settings (e.g. <code>optimizer.nbranch = 4</code>) and the best bound found so far will persist and be used to prune paths on the next call:</p> <pre><code>optimizer.nbranch = 4\npath, path_info = oe.contract_path(eq, *arrays, optimize=optimizer)\n</code></pre>"},{"location":"paths/custom_paths/","title":"Custom Path Optimizers","text":"<p>If you want to implement or just experiment with custom contaction paths then you can easily by subclassing the <code>opt_einsum.paths.PathOptimizer</code> object. For example, imagine we want to test the path that just blindly contracts the first pair of tensors again and again. We would implement this as:</p> <pre><code>import opt_einsum as oe\n\nclass MyOptimizer(oe.paths.PathOptimizer):\n\n    def __call__(self, inputs, output, size_dict, memory_limit=None):\n        return [(0, 1)] * (len(inputs) - 1)\n</code></pre> <p>Once defined we can use this as:</p> <pre><code>import numpy as np\n\n# set-up a random contraction\neq, shapes = oe.helpers.rand_equation(10, 3, seed=42)\narrays = list(map(np.ones, shapes))\n\n# set-up our optimizer and use it\noptimizer = MyOptimizer()\npath, path_info = oe.contract_path(eq, *arrays, optimize=optimizer)\n\nprint(path)\n#&gt; [(0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1), (0, 1)]\n\nprint(path_info.speedup)\n#&gt; 133.21363671496357\n</code></pre> <p>Note that though we still get a considerable speedup over <code>einsum</code> this is of course not a good strategy to take in general.</p>"},{"location":"paths/custom_paths/#custom-random-optimizers","title":"Custom Random Optimizers","text":"<p>If your custom path optimizer is inherently random, then you can reuse all the machinery of the random-greedy approach. Namely:</p> <ul> <li>A max-repeats or max-time approach</li> <li>Minimization with respect to total flops or largest intermediate size</li> <li>Parallelization using a pool-executor</li> </ul> <p>This is done by subclassing the <code>opt_einsum.paths.RandomOptimizer</code>  object and implementing a <code>setup</code> method. Here's an example where we just randomly select any path (again, although we get a considerable speedup over <code>einsum</code> this is not a good strategy to take in general):</p> <pre><code>from opt_einsum.path_random import ssa_path_compute_cost\n\nclass MyRandomOptimizer(oe.path_random.RandomOptimizer):\n\n    @staticmethod\n    def random_path(r, n, inputs, output, size_dict):\n        \"\"\"Picks a completely random contraction order.\n        \"\"\"\n        np.random.seed(r)\n        ssa_path = []\n        remaining = set(range(n))\n        while len(remaining) &gt; 1:\n            i, j = np.random.choice(list(remaining), size=2, replace=False)\n            remaining.add(n + len(ssa_path))\n            remaining.remove(i)\n            remaining.remove(j)\n            ssa_path.append((i, j))\n        cost, size = ssa_path_compute_cost(ssa_path, inputs, output, size_dict)\n        return ssa_path, cost, size\n\n    def setup(self, inputs, output, size_dict):\n        \"\"\"Prepares the function and arguments to repeatedly call.\n        \"\"\"\n        n = len(inputs)\n        trial_fn = self.random_path\n        trial_args = (n, inputs, output, size_dict)\n        return trial_fn, trial_args\n</code></pre> <p>Which we can now instantiate using various other options:</p> <pre><code>optimizer = MyRandomOptimizer(max_repeats=1000, max_time=10,\n                              parallel=True, minimize='size')\npath, path_info = oe.contract_path(eq, *arrays, optimize=optimizer)\n\nprint(path)\n#&gt; [(3, 4), (1, 3), (0, 3), (3, 5), (3, 4), (3, 4), (1, 0), (0, 1), (0, 1)]\n\nprint(path_info.speedup)\n#&gt; 712829.9451056132\n</code></pre> <p>There are a few things to note here:</p> <ol> <li>The core function (<code>MyRandomOptimizer.random_path</code> here), should take a    trial number <code>r</code> as it first argument</li> <li>It should return a ssa_path (see <code>opt_einsum.paths.ssa_to_linear</code> and    <code>opt_einsum.paths.linear_to_ssa</code>) as well as a flops-cost and max-size.</li> <li>The <code>setup</code> method prepares this function, as well as any input to it,    so that the trials will look roughly like    <code>[trial_fn(r, *trial_args) for r in range(max_repeats)]</code>. If you need to    parse the standard arguments (into a network for example), it thus only    needs to be done once per optimization</li> </ol> <p>More details about <code>opt_einsum.paths.RandomOptimizer</code> options can be found in <code>RandomGreedyPathPage</code> section.</p>"},{"location":"paths/dp_path/","title":"The Dynamic Programming Path","text":"<p>The dynamic programming (DP) approach described in reference [1] provides an efficient way to find an asymptotically optimal contraction path by running the following steps:</p> <ol> <li>Compute all traces, i.e. summations over indices occurring exactly in one    input.</li> <li>Decompose the contraction graph of inputs into disconnected subgraphs. Two    inputs are connected if they share at least one summation index.</li> <li>Find the contraction path for each of the disconnected subgraphs using a    DP approach: The optimal contraction path for all sets of <code>n</code> (ranging    from 1 to the number of inputs) connected tensors is found by combining    sets of <code>m</code> and <code>n-m</code> tensors.</li> </ol> <p>Note that computing all the traces in the very beginning can never lead to a non-optimal contraction path.</p> <p>Contractions of disconnected subgraphs can be optimized independently, which still results in an optimal contraction path. However, the computational complexity of finding the contraction path is drastically reduced: If the subgraphs consist of <code>n1</code>, <code>n2</code>, ... inputs, the computational complexity is reduced from <code>O(exp(n1 + n2 + ...))</code> to <code>O(exp(n1) + exp(n2) + ...)</code>.</p> <p>The DP approach will only perform pair contractions and by default will never compute intermediate outer products as in reference [1] it is shown that this always results in an asymptotically optimal contraction path.</p> <p>A major optimization for DP is the cost capping strategy: The DP optimization only memorizes contractions for a subset of inputs, if the total cost for this contraction is smaller than the cost cap. The cost cap is initialized with the minimal possible cost, i.e. the product of all output dimensions, and is iteratively increased by multiplying it with the smallest dimension until a contraction path including all inputs is found.</p> <p>Note that the worst case scaling of DP is exponential in the number of inputs. Nevertheless, if the contraction graph is not completely random, but exhibits a certain kind of structure, it can be used for large contraction graphs and is guaranteed to find an asymptotically optimal contraction path. For this reason it is the most frequently used contraction path optimizer in the field of tensor network states.</p> <p>More specifically, the search is performed over connected subgraphs, which, for example, planar and tree-like graphs have far fewer of. As a rough guide, if the graph is planar, expressions with many tens of tensors are tractable, whereas if the graph is tree-like, expressions with many hundreds of tensors are tractable.</p> <p>[1] Robert N. C. Pfeifer, Jutho Haegeman, and Frank Verstraete Phys. Rev. E 90, 033315 (2014). https://arxiv.org/abs/1304.6112</p>"},{"location":"paths/dp_path/#customizing-the-dynamic-programming-path","title":"Customizing the Dynamic Programming Path","text":"<p>The default <code>optimize='dp'</code> approach has sensible defaults but can be customized with the <code>opt_einsum.paths.DynamicProgramming</code> object.</p> <pre><code>import opt_einsum as oe\n\noptimizer = oe.DynamicProgramming(\n    minimize='size',    # optimize for largest intermediate tensor size\n    search_outer=True,  # search through outer products as well\n    cost_cap=False,     # don't use cost-capping strategy\n)\n\noe.contract(eq, *arrays, optimize=optimizer)\n</code></pre> <p>Warning</p> <p>Note that searching outer products will most likely drastically slow down the optimizer on all but the smallest examples.</p> <p>The values that <code>minimize</code> can take are:</p> <ul> <li><code>'flops'</code>: minimize the total number of scalar operations.</li> <li><code>'size'</code>: minimize the size of the largest intermediate.</li> <li><code>'write'</code>: minimize the combined size of all intermediate tensors -    approximately speaking the amount of memory that will be written. This is    relevant if you were to automatically differentiate through the    contraction, which naively would require storing all intermediates.</li> <li><code>'combo'</code> - minimize <code>flops + alpha * write</code> summed over intermediates, a   default ratio of <code>alpha=64</code> is used, or it can be customized with   <code>f'combo-{alpha}'</code>.</li> <li><code>'limit'</code> - minimize <code>max(flops, alpha * write)</code> summed over intermediates, a   default ratio of <code>alpha=64</code> is used, or it can be customized with <code>f'limit-{alpha}'</code>.</li> </ul> <p>The last two take into account the fact that real contraction performance can be bound by memory speed, and so favor paths with higher arithmetic intensity. The default value of <code>alpha=64</code> is reasonable for both typical CPUs and GPUs.</p>"},{"location":"paths/greedy_path/","title":"The Greedy Path","text":"<p>The <code>'greedy'</code> approach provides a very efficient strategy for finding contraction paths for expressions with large numbers of tensors. It does this by eagerly choosing contractions in three stages:</p> <ol> <li>Eagerly compute any Hadamard products (in arbitrary order -- this is    commutative).</li> <li>Greedily contract pairs of remaining tensors, at each step choosing the    pair that maximizes <code>reduced_size</code> -- these are generally inner    products.</li> <li>Greedily compute any pairwise outer products, at each step choosing    the pair that minimizes <code>sum(input_sizes)</code>.</li> </ol> <p>The cost heuristic <code>reduced_size</code> is simply the size of the pair of potential tensors to be contracted, minus the size of the resulting tensor.</p> <p>The <code>greedy</code> algorithm has space and time complexity <code>O(n * k)</code> where <code>n</code> is the number of input tensors and <code>k</code> is the maximum number of tensors that share any dimension (excluding dimensions that occur in the output or in every tensor). As such, the algorithm scales well to very large sparse contractions of low-rank tensors, and indeed, often finds the optimal, or close to optimal path in such cases.</p> <p>The <code>greedy</code> functionality is provided by <code>opt_einsum.paths.greedy</code>, and is selected by the default <code>optimize='auto'</code> mode of <code>opt_einsum</code> for expressions with many inputs. Expressions of up to a thousand tensors should still take well less than a second to find paths for.</p>"},{"location":"paths/greedy_path/#optimal-scaling-misses","title":"Optimal Scaling Misses","text":"<p>The greedy algorithm, while inexpensive, can occasionally miss optimal scaling in some circumstances as seen below. The <code>greedy</code> algorithm prioritizes expressions which remove the largest indices first, in this particular case this is the incorrect choice and it is difficult for any heuristic algorithm to \"see ahead\" as would be needed here.</p> <p>It should be stressed these cases are quite rare and by default <code>contract</code> uses the <code>optimal</code> path for four and fewer inputs as the cost of evaluating the <code>optimal</code> path is similar to that of the <code>greedy</code> path. Similarly, for 5-8 inputs, <code>contract</code> uses one of the branching strategies which can find higher quality paths.</p> <pre><code>M = np.random.rand(35, 37, 59)\nA = np.random.rand(35, 51, 59)\nB = np.random.rand(37, 51, 51, 59)\nC = np.random.rand(59, 27)\n\npath, desc = oe.contract_path('xyf,xtf,ytpf,fr-&gt;tpr', M, A, B, C, optimize=\"greedy\")\nprint(desc)\n#&gt;   Complete contraction:  xyf,xtf,ytpf,fr-&gt;tpr\n#&gt;          Naive scaling:  6\n#&gt;      Optimized scaling:  5\n#&gt;       Naive FLOP count:  2.146e+10\n#&gt;   Optimized FLOP count:  4.165e+08\n#&gt;    Theoretical speedup:  51.533\n#&gt;   Largest intermediate:  5.371e+06 elements\n#&gt; --------------------------------------------------------------------------------\n#&gt; scaling        BLAS                current                             remaining\n#&gt; --------------------------------------------------------------------------------\n#&gt;    5          False         ytpf,xyf-&gt;tpfx                      xtf,fr,tpfx-&gt;tpr\n#&gt;    4          False          tpfx,xtf-&gt;tpf                           fr,tpf-&gt;tpr\n#&gt;    4           GEMM            tpf,fr-&gt;tpr                              tpr-&gt;tpr\n\npath, desc = oe.contract_path('xyf,xtf,ytpf,fr-&gt;tpr', M, A, B, C, optimize=\"optimal\")\nprint(desc)\n#&gt;   Complete contraction:  xyf,xtf,ytpf,fr-&gt;tpr\n#&gt;          Naive scaling:  6\n#&gt;      Optimized scaling:  4\n#&gt;       Naive FLOP count:  2.146e+10\n#&gt;   Optimized FLOP count:  2.744e+07\n#&gt;    Theoretical speedup:  782.283\n#&gt;   Largest intermediate:  1.535e+05 elements\n#&gt; --------------------------------------------------------------------------------\n#&gt; scaling        BLAS                current                             remaining\n#&gt; --------------------------------------------------------------------------------\n#&gt;    4          False           xtf,xyf-&gt;tfy                      ytpf,fr,tfy-&gt;tpr\n#&gt;    4          False          tfy,ytpf-&gt;tfp                           fr,tfp-&gt;tpr\n#&gt;    4           TDOT            tfp,fr-&gt;tpr                              tpr-&gt;tpr\n</code></pre> <p>So we can see that the <code>greedy</code> algorithm finds a path which is about 16 times slower than the <code>optimal</code> one. In such cases, it might be worth using one of the more exhaustive optimization strategies: <code>'optimal'</code>, <code>'branch-all'</code> or <code>branch-2</code> (all of which will find the optimal path in this example).</p>"},{"location":"paths/greedy_path/#customizing-the-greedy-path","title":"Customizing the Greedy Path","text":"<p>The greedy path is a local optimizer in that it only ever assesses pairs of tensors to contract, assigning each a heuristic 'cost' and then choosing the 'best' of these. Custom greedy approaches can be implemented by supplying callables to the <code>cost_fn</code> and <code>choose_fn</code> arguments of <code>opt_einsum.paths.greedy</code>.</p>"},{"location":"paths/introduction/","title":"Introduction","text":"<p>Performing an optimized tensor contraction to speed up <code>einsum</code> involves two key stages:</p> <ol> <li>Finding a pairwise contraction order, or 'path'.</li> <li>Performing the sequence of contractions given this path.</li> </ol> <p>The better the quality of path found in the first step, the quicker the actual contraction in the second step can be -- often dramatically. However, finding the optimal path is an NP-hard problem that can quickly become intractable, meaning that a  balance must be struck between the time spent finding a path, and its quality. <code>opt_einsum</code> handles this by using several path finding algorithms, which can be manually specified using the <code>optimize</code> keyword. These are:</p> <ul> <li>The <code>'optimal'</code> strategy - an exhaustive search of all possible paths</li> <li>The <code>'dynamic-programming'</code> strategy - a near-optimal search based off dynamic-programming</li> <li>The <code>'branch'</code> strategy - a more restricted search of many likely paths</li> <li>The <code>'greedy'</code> strategy - finds a path one step at a time using a cost   heuristic</li> </ul> <p>By default (<code>optimize='auto'</code>), <code>opt_einsum.contract</code> will select the best of these it can while aiming to keep path finding times below around 1ms. An analysis of each of these approaches' performance can be found at the bottom of this page.</p> <p>For large and complex contractions, there is the <code>'random-greedy'</code> approach, which samples many (by default 32) greedy paths and can be customized to explicitly spend a maximum amount of time searching. Another preset, <code>'random-greedy-128'</code>, uses 128 paths for a more exhaustive search. See <code>RandomGreedyPath</code> page for more details on configuring these.</p> <p>Finally, there is the <code>'auto-hq'</code> preset which targets a much larger search time (~1sec) in return for finding very high quality paths, dispatching to the <code>'optimal'</code>, <code>'dynamic-programming'</code> and then <code>'random-greedy-128'</code> paths depending on contraction size.</p> <p>If you want to find the path separately to performing the contraction, or just inspect information about the path found, you can use the function <code>opt_einsum.contract_path</code>.</p>"},{"location":"paths/introduction/#examining-the-path","title":"Examining the Path","text":"<p>As an example, consider the following expression found in a perturbation theory (one of ~5,000 such expressions):</p> <pre><code>'bdik,acaj,ikab,ajac,ikbd'\n</code></pre> <p>At first, it would appear that this scales like N^7 as there are 7 unique indices; however, we can define a intermediate to reduce this scaling.</p> <pre><code># (N^5 scaling)\na = 'bdik,ikab,ikbd'\n\n# (N^4 scaling)\nresult = 'acaj,ajac,a'\n</code></pre> <p>This is a single possible path to the final answer (and notably, not the most optimal) out of many possible paths. Now, let opt_einsum compute the optimal path:</p> <pre><code>import opt_einsum as oe\n\n# Take a complex string\neinsum_string = 'bdik,acaj,ikab,ajac,ikbd-&gt;'\n\n# Build random views to represent this contraction\nunique_inds = set(einsum_string) - {',', '-', '&gt;'}\nindex_size = [10, 17, 9, 10, 13, 16, 15, 14, 12]\nsizes_dict = dict(zip(unique_inds, index_size))\nviews = oe.helpers.build_views(einsum_string, sizes_dict)\n\npath, path_info = oe.contract_path(einsum_string, *views)\n\nprint(path)\n#&gt; [(0, 4), (1, 3), (0, 1), (0, 1)]\n\nprint(path_info)\n#&gt;   Complete contraction:  bdik,acaj,ikab,ajac,ikbd-&gt;\n#&gt;          Naive scaling:  7\n#&gt;      Optimized scaling:  4\n#&gt;       Naive FLOP count:  2.387e+8\n#&gt;   Optimized FLOP count:  8.068e+4\n#&gt;    Theoretical speedup:  2958.354\n#&gt;   Largest intermediate:  1.530e+3 elements\n#&gt; --------------------------------------------------------------------------------\n#&gt; scaling        BLAS                current                             remaining\n#&gt; --------------------------------------------------------------------------------\n#&gt;    4              0         ikbd,bdik-&gt;ikb                  acaj,ikab,ajac,ikb-&gt;\n#&gt;    4    GEMV/EINSUM            ikb,ikab-&gt;a                         acaj,ajac,a-&gt;\n#&gt;    3              0           ajac,acaj-&gt;a                                 a,a-&gt;\n#&gt;    1            DOT                  a,a-&gt;                                    -&gt;\n</code></pre> <p>We can then check that actually performing the contraction produces the expected result:</p> <pre><code>import numpy as np\n\neinsum_result = np.einsum(\"bdik,acaj,ikab,ajac,ikbd-&gt;\", *views)\ncontract_result = oe.contract(\"bdik,acaj,ikab,ajac,ikbd-&gt;\", *views)\n\nnp.allclose(einsum_result, contract_result)\n#&gt; True\n</code></pre> <p>By contracting terms in the correct order we can see that this expression can be computed with N^4 scaling. Even with the overhead of finding the best order or 'path' and small dimensions, <code>opt_einsum</code> is roughly 3000 times faster than pure einsum for this expression.</p>"},{"location":"paths/introduction/#format-of-the-path","title":"Format of the Path","text":"<p>Let us look at the structure of a canonical <code>einsum</code> path found in NumPy and its optimized variant:</p> <pre><code>einsum_path = [(0, 1, 2, 3, 4)]\nopt_path = [(1, 3), (0, 2), (0, 2), (0, 1)]\n</code></pre> <p>In opt_einsum each element of the list represents a single contraction. In the above example the einsum_path would effectively compute the result as a single contraction identical to that of <code>einsum</code>, while the opt_path would perform four contractions in order to reduce the overall scaling. The first tuple in the opt_path, <code>(1,3)</code>, pops the second and fourth terms, then contracts them together to produce a new term which is then appended to the list of terms, this is continued until all terms are contracted. An example should illuminate this:</p> <pre><code>---------------------------------------------------------------------------------\nscaling   GEMM                   current                                remaining\n---------------------------------------------------------------------------------\nterms = ['bdik', 'acaj', 'ikab', 'ajac', 'ikbd'] contraction = (1, 3)\n  3     False              ajac,acaj-&gt;a                       bdik,ikab,ikbd,a-&gt;\nterms = ['bdik', 'ikab', 'ikbd', 'a'] contraction = (0, 2)\n  4     False            ikbd,bdik-&gt;bik                             ikab,a,bik-&gt;\nterms = ['ikab', 'a', 'bik'] contraction = (0, 2)\n  4     False              bik,ikab-&gt;a                                    a,a-&gt;\nterms = ['a', 'a'] contraction = (0, 1)\n  1       DOT                    a,a-&gt;                                       -&gt;\n</code></pre> <p>A path specified in this format can explicitly be supplied directly to <code>opt_einsum.contract</code> using the <code>optimize</code> keyword:</p> <pre><code>contract_result = oe.contract(\"bdik,acaj,ikab,ajac,ikbd-&gt;\", *views, optimize=opt_path)\n\nnp.allclose(einsum_result, contract_result)\n#&gt; True\n</code></pre>"},{"location":"paths/introduction/#performance-comparison","title":"Performance Comparison","text":"<p>The following graphs should give some indication of the tradeoffs between path finding time and path quality. They are generated by finding paths with each possible algorithm for many randomly generated networks of <code>n</code> tensors with varying connectivity.</p> <p>First we have the time to find each path as a function of the number of terms in the expression:</p> <p></p> <p>Clearly the exhaustive (<code>'optimal'</code>, <code>'branch-all'</code>) and exponential (<code>'branch-2'</code>) searches eventually scale badly, but for modest amounts of terms they incur only a small overhead. The <code>'random-greedy'</code> approach is not shown here as it is simply <code>max_repeats</code> times slower than the <code>'greedy'</code> approach - at least if not parallelized.</p> <p>Next we can look at the average FLOP speedup (as compared to the easiest path to find, <code>'greedy'</code>):</p> <p></p> <p>One can see that the hierarchy of path qualities is:</p> <ol> <li><code>'optimal'</code> (used by auto for <code>n &lt;= 4</code>)</li> <li><code>'branch-all'</code> (used by auto for <code>n &lt;= 6</code>)</li> <li><code>'branch-2'</code> (used by auto for <code>n &lt;= 8</code>)</li> <li><code>'branch-1'</code> (used by auto for <code>n &lt;= 14</code>)</li> <li><code>'greedy'</code> (used by auto for anything larger)</li> </ol> <p>Note</p> <p>The performance of the <code>'random=greedy'</code> approach (which is never used automatically) can be found separately in <code>RandomGreedyPath</code> section.</p> <p>There are a few important caveats to note with this graph. Firstly, the benefits of more advanced path finding are very dependent on the complexity of the expression. For 'simple' contractions, all the different approaches will mostly find the same path (as here). However, for 'tricky' contractions, there will be certain cases where the more advanced algorithms will find much better paths. As such, while this graph gives a good idea of the relative performance of each algorithm, the 'average speedup' is not a perfect indicator since worst-case performance might be more critical.</p> <p>Note that the speedups for any of the methods as compared to a standard <code>einsum</code> or a naively chosen path (such as <code>path=[(0, 1), (0, 1), ...]</code>) are all exponentially large and not shown.</p>"},{"location":"paths/optimal_path/","title":"The Optimal Path","text":"<p>The most optimal path can be found by searching through every possible way to contract the tensors together, this includes all combinations with the new intermediate tensors as well. While this algorithm scales like N!, and can often become more costly to compute than the unoptimized contraction itself, it provides an excellent benchmark. The function that computes this path in opt_einsum is called <code>opt_einsum.paths.optimal</code> and works by performing a recursive, depth-first search. By keeping track of the best path found so far, in terms of total estimated FLOP count, the search can then quickly prune many paths as soon as as they exceed this best. This optimal strategy is used by default with the <code>optimize='auto'</code> mode of <code>opt_einsum</code> for 4 tensors or less, though it can handle expressions of up to 9-10 tensors in a matter of seconds.</p> <p>Let us look at an example:</p> <pre><code>Contraction:  abc,dc,ac-&gt;bd\n</code></pre> <p>Build a list with tuples that have the following form:</p> <pre><code>#&gt; iteration 0:\n#&gt;  \"(cost, path,  list of input sets remaining)\"\n#&gt; [ (0,    [],    [set(['a', 'c', 'b']), set(['d', 'c']), set(['a', 'c'])] ]\n</code></pre> <p>Since this is iteration zero, we have the initial list of input sets. We can consider three possible combinations where we contract list positions (0, 1), (0, 2), or (1, 2) together:</p> <pre><code>#&gt; iteration 1:\n#&gt; [ (9504, [(0, 1)], [set(['a', 'c']), set(['a', 'c', 'b', 'd'])  ]),\n#&gt;   (1584, [(0, 2)], [set(['c', 'd']), set(['c', 'b'])            ]),\n#&gt;   (864,  [(1, 2)], [set(['a', 'c', 'b']), set(['a', 'c', 'd'])  ])]\n</code></pre> <p>We have now run through the three possible combinations, computed the cost of the contraction up to this point, and appended the resulting indices from the contraction to the list. As all contractions only have two remaining input sets the only possible contraction is (0, 1):</p> <pre><code>#&gt; iteration 2:\n#&gt; [ (28512, [(0, 1), (0, 1)], [set(['b', 'd'])  ]),\n#&gt;   (3168,  [(0, 2), (0, 1)], [set(['b', 'd'])  ]),\n#&gt;   (19872, [(1, 2), (0, 1)], [set(['b', 'd'])  ])]\n</code></pre> <p>The final contraction cost is computed, and we choose the second path from the list as the overall cost is the lowest.</p>"},{"location":"paths/random_greedy_path/","title":"The Random-Greedy Path","text":"<p>For large and complex contractions the exhaustive approaches will be too slow while the greedy path might be very far from optimal. In this case you might want to consider the <code>'random-greedy'</code> path optimizer. This samples many greedy paths and selects the best one found, which can often be exponentially better than the average.</p> <pre><code>import opt_einsum as oe\nimport numpy as np\nimport math\n\neq, shapes = oe.helpers.rand_equation(40, 5, seed=1, d_max=2)\narrays = list(map(np.ones, shapes))\n\npath_greedy = oe.contract_path(eq, *arrays, optimize='greedy')[1]\nprint(math.log2(path_greedy.opt_cost))\n#&gt; 36.04683022558587\n\npath_rand_greedy = oe.contract_path(eq, *arrays, optimize='random-greedy')[1]\nprint(math.log2(path_rand_greedy.opt_cost))\n#&gt; 32.203616699170865\n</code></pre> <p>So here the random-greedy approach has found a path about 16 times quicker (<code>= 2^(36 - 32)</code>).</p> <p>This approach works by randomly choosing from the best <code>n</code> contractions at each step, weighted by a Boltzmann factor with respect to the contraction with the 'best' cost. As such, contractions with very similar costs will be explored with equal probability, whereas those with higher costs will be less likely, but still possible. In this way, the optimizer can randomly explore the huge space of possible paths, but in a guided manner.</p> <p>The following graph roughly demonstrates the potential benefits of the <code>'random-greedy'</code> algorithm, here for large randomly generated contractions, with either 8, 32 (the default), or 128 repeats:</p> <p></p> <p>Note</p> <p>Bear in mind that such speed-ups are not guaranteed - it very much depends on how structured or complex your contractions are.</p>"},{"location":"paths/random_greedy_path/#customizing-the-random-greedy-path","title":"Customizing the Random-Greedy Path","text":"<p>The random-greedy optimizer can be customized by instantiating your own <code>opt_einsum.paths.RandomGreedy</code> object. Here you can control:</p> <ul> <li><code>temperature</code> - how far to stray from the locally 'best' contractions</li> <li><code>rel_temperature</code> - whether to normalize the temperature</li> <li><code>nbranch</code> - how many contractions (branches) to consider at each step</li> <li><code>cost_fn</code> - how to cost potential contractions</li> </ul> <p>There are also the main <code>opt_einsum.paths.RandomOptimizer</code> options:</p> <ul> <li><code>max_repeats</code> - the maximum number of repeats</li> <li><code>max_time</code> - the maximum amount of time to run for (in seconds)</li> <li><code>minimize</code> - whether to minimize for total <code>'flops'</code> or <code>'size'</code> of the   largest intermediate</li> </ul> <p>For example, here we'll create an optimizer, then change its temperature whilst reusing it. We'll also set a high <code>max_repeats</code> and instead use a maximum time to terminate the search:</p> <pre><code>optimizer = oe.RandomGreedy(max_time=2, max_repeats=1_000_000)\n\nfor T in [1000, 100, 10, 1, 0.1]:\n    optimizer.temperature = T\n    path_rand_greedy = oe.contract_path(eq, *arrays, optimize=optimizer)[1]\n    print(math.log2(optimizer.best['flops']))\n\n#&gt; 32.81709395639357\n#&gt; 32.67625007170783\n#&gt; 31.719756871539033\n#&gt; 31.62043317835677\n#&gt; 31.253305891247\n\n# the total number of trials so far\nprint(len(optimizer.costs))\n#&gt; 2555\n</code></pre> <p>So we have improved a bit on the standard <code>'random-greedy'</code> (which performs 32 repeats by default). The <code>optimizer</code> object now stores both the best path found so far - <code>optimizer.path</code> - as well as the list of flop-costs and maximum sizes found for each trial - <code>optimizer.costs</code> and <code>optimizer.sizes</code> respectively.</p>"},{"location":"paths/random_greedy_path/#parallelizing-the-random-greedy-search","title":"Parallelizing the Random-Greedy Search","text":"<p>Since each greedy attempt is independent, the random-greedy approach is naturally suited to parallelization. This can be automatically handled by specifying the <code>parallel</code> keyword like so:</p> <pre><code># use same number of processes as cores\noptimizer = oe.RandomGreedy(parallel=True)\n\n# or use specific number of processes\noptimizer = oe.RandomGreedy(parallel=4)\n</code></pre> <p>Warning</p> <p>The pool-executor used to perform this parallelization is the <code>ProcessPoolExecutor</code> from the <code>concurrent.futures</code>  module.</p> <p>For full control over the parallelization you can supply any pool-executor like object, which should have an API matching the Python 3 concurrent.futures module:</p> <pre><code>from concurrent.futures import ProcessPoolExecutor\n\npool = ProcessPoolExecutor()\noptimizer = oe.RandomGreedy(parallel=pool, max_repeats=128)\npath_rand_greedy = oe.contract_path(eq, *arrays, optimize=optimizer)[1]\n\nprint(math.log2(optimizer.best['flops']))\n#&gt; 31.64992600300931\n</code></pre> <p>Other examples of such pools include:</p> <ul> <li>loky</li> <li>dask.distributed</li> <li>mpi4py</li> </ul>"}]}